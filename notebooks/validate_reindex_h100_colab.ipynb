{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lovli Source-Gating Validation Run (Colab GPU)\n",
        "\n",
        "This notebook runs the **v3 source-gating workflow** on Colab (H100/T4 compatible), so we avoid local RAM limits.\n",
        "\n",
        "It runs:\n",
        "- `scripts/build_catalog.py` (merge `data/nl` + `data/sf`)\n",
        "- `scripts/validate_reindex.py`\n",
        "- `scripts/analyze_law_contamination.py`\n",
        "- `scripts/sweep_retrieval_thresholds.py`\n",
        "\n",
        "The setup enables law routing + law coherence filtering, then exports analysis artifacts for review."
      ],
      "id": "d2ab62ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Runtime and Repository Setup\n",
        "\n",
        "Use a **GPU runtime** before running this notebook (H100 preferred, T4 supported).\n",
        "\n",
        "If you cloned with an older commit, restart runtime and rerun from the top."
      ],
      "id": "4f0df554"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content\n",
        "!rm -rf lovli\n",
        "!git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Install project with dependencies required by validation scripts.\n",
        "%pip install -q -U pip\n",
        "%pip install -q -e .\n",
        "\n",
        "# Safety net for environments where editable install path is delayed.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "src_path = str(Path('/content/lovli/src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "print('Setup complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f'GPU: {name}')\n",
        "    print(f'VRAM: {props.total_memory / (1024**3):.1f} GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Configuration (v3 + routing/coherence)"
      ],
      "id": "4b803b9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Required Qdrant settings\n",
        "os.environ['QDRANT_URL'] = 'https://acc5c492-7d2c-4b95-b0c5-2931ff2ecebd.eu-west-1-0.aws.cloud.qdrant.io'\n",
        "os.environ['QDRANT_API_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.mpjUx25tLCPdDaEY31eqC1YcVBQijf2Tl4xru1F_56k'\n",
        "os.environ['QDRANT_COLLECTION_NAME'] = 'lovli_laws_v3'\n",
        "\n",
        "# Required by Settings model even for retrieval/eval scripts.\n",
        "os.environ['OPENROUTER_API_KEY'] = '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'\n",
        "\n",
        "# Keep traces off for speed/clean logs.\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
        "os.environ['LANGSMITH_TRACING'] = 'false'\n",
        "os.environ['SWEEP_SKIP_INDEX_SCAN'] = 'true'\n",
        "\n",
        "# Versioned trust profiles: switch TRUST_PROFILE between balanced_v1 and strict_v1.\n",
        "os.environ['TRUST_PROFILE_VERSION'] = '2026-02-16'\n",
        "profile_name = os.environ.get('TRUST_PROFILE', 'balanced_v1')\n",
        "profiles = {\n",
        "    'balanced_v1': {\n",
        "        'RETRIEVAL_K_INITIAL': '20',\n",
        "        'RERANKER_CONFIDENCE_THRESHOLD': '0.35',\n",
        "        'RERANKER_MIN_DOC_SCORE': '0.35',\n",
        "        'RERANKER_AMBIGUITY_MIN_GAP': '0.05',\n",
        "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
        "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'true',\n",
        "    },\n",
        "    'strict_v1': {\n",
        "        'RETRIEVAL_K_INITIAL': '15',\n",
        "        'RERANKER_CONFIDENCE_THRESHOLD': '0.45',\n",
        "        'RERANKER_MIN_DOC_SCORE': '0.55',\n",
        "        'RERANKER_AMBIGUITY_MIN_GAP': '0.10',\n",
        "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
        "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'false',\n",
        "    },\n",
        "}\n",
        "profile = profiles.get(profile_name, profiles['balanced_v1'])\n",
        "os.environ['TRUST_PROFILE'] = profile_name if profile_name in profiles else 'balanced_v1'\n",
        "\n",
        "# Shared law routing and coherence settings.\n",
        "os.environ['LAW_ROUTING_ENABLED'] = 'true'\n",
        "os.environ['LAW_CATALOG_PATH'] = 'data/law_catalog.json'\n",
        "os.environ['LAW_ROUTING_PREFILTER_K'] = '80'\n",
        "os.environ['LAW_ROUTING_RERANK_TOP_K'] = '6'\n",
        "os.environ['LAW_ROUTING_MIN_CONFIDENCE'] = '0.30'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'] = '0.55'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'] = '0.04'\n",
        "os.environ['LAW_ROUTING_FALLBACK_MAX_LAWS'] = '12'\n",
        "os.environ['LAW_COHERENCE_FILTER_ENABLED'] = 'true'\n",
        "os.environ['LAW_COHERENCE_MIN_LAW_COUNT'] = '2'\n",
        "os.environ['LAW_COHERENCE_SCORE_GAP'] = '0.15'\n",
        "os.environ['LAW_COHERENCE_RELATIVE_GAP'] = '0.05'\n",
        "os.environ['LAW_COHERENCE_MAX_SCORE_WEIGHT'] = '0.6'\n",
        "os.environ['LAW_COHERENCE_MIN_KEEP'] = '1'\n",
        "os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'] = '0.60'\n",
        "\n",
        "# Apply selected profile values.\n",
        "for key, value in profile.items():\n",
        "    os.environ[key] = value\n",
        "\n",
        "# Guard against accidental string values like 'None'.\n",
        "raw = os.environ.get('SWEEP_SAMPLE_SIZE')\n",
        "if raw is not None and raw.strip().lower() in {'', 'none', 'null'}:\n",
        "    os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "\n",
        "print('TRUST_PROFILE          =', os.environ['TRUST_PROFILE'])\n",
        "print('TRUST_PROFILE_VERSION  =', os.environ['TRUST_PROFILE_VERSION'])\n",
        "print('QDRANT_COLLECTION_NAME =', os.environ['QDRANT_COLLECTION_NAME'])\n",
        "print('LAW_ROUTING_ENABLED    =', os.environ['LAW_ROUTING_ENABLED'])\n",
        "print('LAW_CATALOG_PATH       =', os.environ['LAW_CATALOG_PATH'])\n",
        "print('LAW_ROUTING_PREFILTER  =', os.environ['LAW_ROUTING_PREFILTER_K'])\n",
        "print('LAW_ROUTING_RERANK_K   =', os.environ['LAW_ROUTING_RERANK_TOP_K'])\n",
        "print('LAW_ROUTING_CONF_MIN   =', os.environ['LAW_ROUTING_MIN_CONFIDENCE'])\n",
        "print('LAW_ROUTE_UNCERT_CEIL  =', os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'])\n",
        "print('LAW_ROUTE_UNCERT_GAP   =', os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'])\n",
        "print('LAW_ROUTE_FALLBACK     =', os.environ['LAW_ROUTING_FALLBACK_UNFILTERED'])\n",
        "print('LAW_COHERENCE_FILTER   =', os.environ['LAW_COHERENCE_FILTER_ENABLED'])\n",
        "print('LAW_COHERENCE_CONC_THR =', os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'])\n",
        "print('SWEEP_SAMPLE_SIZE      =', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ce9f311d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional quick mode before full run.\n",
        "# Uncomment to run a small sample first.\n",
        "# os.environ['SWEEP_SAMPLE_SIZE'] = '100'\n",
        "\n",
        "# Ensure full run by default.\n",
        "os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "print('SWEEP_SAMPLE_SIZE now:', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mount Drive, Extract Data, Build Catalog, Validate Reindex"
      ],
      "id": "ce068d4d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def run_to_log(cmd: str, log_path: Path) -> int:\n",
        "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(log_path, 'w', encoding='utf-8') as log_file:\n",
        "        proc = subprocess.run(\n",
        "            cmd,\n",
        "            shell=True,\n",
        "            cwd='/content/lovli',\n",
        "            stdout=log_file,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "        )\n",
        "    return proc.returncode\n",
        "\n",
        "\n",
        "def print_log_matches(log_path: Path, patterns: list[str], limit: int = 50) -> None:\n",
        "    if not log_path.exists():\n",
        "        print(f'log missing: {log_path}')\n",
        "        return\n",
        "    lines = log_path.read_text(encoding='utf-8', errors='ignore').splitlines()\n",
        "    kept = []\n",
        "    for line in lines:\n",
        "        if any(p in line for p in patterns):\n",
        "            kept.append(line)\n",
        "    print(f'--- {log_path.name} (key lines) ---')\n",
        "    for line in kept[-limit:]:\n",
        "        print(line)\n",
        "\n",
        "\n",
        "# Mount Drive for access to the compressed dataset.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update this path if your tar is moved.\n",
        "tar_path = Path('/content/drive/MyDrive/Colab Notebooks/Lovli/data/lovli-data.tar.bz2')\n",
        "assert tar_path.exists(), f'Data tar not found: {tar_path}'\n",
        "\n",
        "# Extract into repo data/ folder (safe to rerun).\n",
        "subprocess.run(\"mkdir -p /content/lovli/data\", shell=True, check=True)\n",
        "subprocess.run(\n",
        "    \"tar -xjf '/content/drive/MyDrive/Colab Notebooks/Lovli/data/lovli-data.tar.bz2' -C /content/lovli --exclude='._*'\",\n",
        "    shell=True,\n",
        "    check=True,\n",
        ")\n",
        "\n",
        "nl_count = len(list(Path('/content/lovli/data/nl').glob('*.xml')))\n",
        "sf_count = len(list(Path('/content/lovli/data/sf').glob('*.xml')))\n",
        "print({'nl_xml_files': nl_count, 'sf_xml_files': sf_count})\n",
        "assert nl_count > 0 and sf_count > 0, 'Expected both data/nl and data/sf to contain XML files.'\n",
        "\n",
        "# Build merged catalog used by law routing (no summaries for speed).\n",
        "build_log = Path('/content/lovli/eval/logs/build_catalog.log')\n",
        "rc = run_to_log(\n",
        "    'python scripts/build_catalog.py data/nl data/sf --no-summaries --output data/law_catalog.json',\n",
        "    build_log,\n",
        ")\n",
        "print('build_catalog exit_code =', rc)\n",
        "print_log_matches(\n",
        "    build_log,\n",
        "    patterns=['Catalog build complete', 'Laws cataloged', 'With summaries', 'Missing summaries', 'Output:', 'Time:'],\n",
        ")\n",
        "assert rc == 0, 'build_catalog failed; inspect eval/logs/build_catalog.log'\n",
        "\n",
        "# Validate metadata + retrieval smoke checks on v3 collection.\n",
        "validate_log = Path('/content/lovli/eval/logs/validate_reindex.log')\n",
        "rc = run_to_log(\n",
        "    'python scripts/validate_reindex.py --collection lovli_laws_v3 --with-smoke',\n",
        "    validate_log,\n",
        ")\n",
        "print('validate_reindex exit_code =', rc)\n",
        "print_log_matches(\n",
        "    validate_log,\n",
        "    patterns=['Collection:', 'total_points=', 'missing_doc_type=', 'smoke query=', 'Validation completed.'],\n",
        ")\n",
        "assert rc == 0, 'validate_reindex failed; inspect eval/logs/validate_reindex.log'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f478d940"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Law Contamination Analysis"
      ],
      "id": "308b8d5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "contam_log = Path('/content/lovli/eval/logs/analyze_law_contamination.log')\n",
        "rc = run_to_log(\n",
        "    'python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json',\n",
        "    contam_log,\n",
        ")\n",
        "print('analyze_law_contamination exit_code =', rc)\n",
        "print_log_matches(\n",
        "    contam_log,\n",
        "    patterns=['Processed', 'Saved contamination report', 'Contamination rate='],\n",
        ")\n",
        "\n",
        "report_path = Path('/content/lovli/eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('--- contamination aggregate ---')\n",
        "    for key in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'{key}: {agg.get(key)}')\n",
        "\n",
        "assert rc == 0, 'analyze_law_contamination failed; inspect eval/logs/analyze_law_contamination.log'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6db4a328"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional quick check before full sweep.\n",
        "# Use a small sample to verify config quickly.\n",
        "%cd /content/lovli\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ['SWEEP_SAMPLE_SIZE'] = '10'\n",
        "quick_sweep_log = Path('/content/lovli/eval/logs/retrieval_sweep_quick.log')\n",
        "rc = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', quick_sweep_log)\n",
        "os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "\n",
        "print('quick sweep exit_code =', rc)\n",
        "print_log_matches(\n",
        "    quick_sweep_log,\n",
        "    patterns=['Using sample size', 'Saved results:', 'Top 5 configurations:'],\n",
        ")\n",
        "\n",
        "sweep_path = Path('/content/lovli/eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('--- quick sweep top row ---')\n",
        "        for key in [\n",
        "            'is_profile_default_row',\n",
        "            'recall_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'law_contamination_rate',\n",
        "            'balanced_score',\n",
        "        ]:\n",
        "            print(f'{key}: {top.get(key)}')\n",
        "\n",
        "assert rc == 0, 'quick sweep failed; inspect eval/logs/retrieval_sweep_quick.log'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a30a858c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Full Retrieval Sweep (Colab run)"
      ],
      "id": "09926968"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "full_sweep_log = Path('/content/lovli/eval/logs/retrieval_sweep_full.log')\n",
        "rc = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', full_sweep_log)\n",
        "\n",
        "print('full sweep exit_code =', rc)\n",
        "print_log_matches(\n",
        "    full_sweep_log,\n",
        "    patterns=['Saved results:', 'Top 5 configurations:'],\n",
        ")\n",
        "\n",
        "sweep_path = Path('/content/lovli/eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('--- full sweep top row ---')\n",
        "        for key in [\n",
        "            'is_profile_default_row',\n",
        "            'recall_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "            'balanced_score',\n",
        "        ]:\n",
        "            print(f'{key}: {top.get(key)}')\n",
        "\n",
        "assert rc == 0, 'full sweep failed; inspect eval/logs/retrieval_sweep_full.log'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3e31d5ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Must-pass regression gates against versioned baseline.\n",
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "\n",
        "gates_log = Path('/content/lovli/eval/logs/regression_gates.log')\n",
        "rc = run_to_log(\n",
        "    'python -u scripts/check_regression_gates.py '\n",
        "    '--contamination-report eval/law_contamination_report.json '\n",
        "    '--sweep-results eval/retrieval_sweep_results.json '\n",
        "    '--baseline eval/baselines/production_trust_baseline_v1.json '\n",
        "    '--profile \"$TRUST_PROFILE\"',\n",
        "    gates_log,\n",
        ")\n",
        "print('regression gates exit_code =', rc)\n",
        "print_log_matches(\n",
        "    gates_log,\n",
        "    patterns=['Gate sweep row selected', '[PASS]', '[FAIL]', 'Gate checks passed', 'All regression gates passed'],\n",
        "    limit=120,\n",
        ")\n",
        "assert rc == 0, 'regression gates failed; inspect eval/logs/regression_gates.log'"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "cbf086b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Artifact Overview and Quick Metric Check\n",
        "\n",
        "Run acceptance targets (balanced objective):\n",
        "- `recall_at_k` should improve materially vs previous baseline (~0.146)\n",
        "- `citation_precision` should increase from previous baseline (~0.073)\n",
        "- `unexpected_citation_rate` should decrease\n",
        "- `law_coherence_filtered_count` should be non-zero on full sweep\n",
        "- `missing_doc_type` must remain `0`"
      ],
      "id": "1a061fef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!ls -lah eval\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts = [\n",
        "    Path('data/law_catalog.json'),\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "]\n",
        "for p in artifacts:\n",
        "    print(f'{p}:', 'exists' if p.exists() else 'missing')\n",
        "\n",
        "report_path = Path('eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('\\nContamination aggregate:')\n",
        "    for k in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'  {k}: {agg.get(k)}')\n",
        "\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('\\nTop sweep row:')\n",
        "        for k in [\n",
        "            'recall_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "        ]:\n",
        "            print(f'  {k}: {top.get(k)}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9d1009eb"
    }
  ],
  "metadata": {
    "colab": {
      "name": "validate_reindex_h100_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}