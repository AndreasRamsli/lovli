{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2ab62ee",
   "metadata": {},
   "source": [
    "# Lovli Source-Gating Validation Run (Colab GPU)\n",
    "\n",
    "This notebook runs the **v3 source-gating workflow** on Colab (H100/T4 compatible), so we avoid local RAM limits.\n",
    "\n",
    "It runs:\n",
    "- `scripts/build_catalog.py` (merge `data/nl` + `data/sf`)\n",
    "- `scripts/validate_reindex.py`\n",
    "- `scripts/analyze_law_contamination.py`\n",
    "- `scripts/sweep_retrieval_thresholds.py`\n",
    "\n",
    "The setup enables law routing + law coherence filtering, then exports analysis artifacts for review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0df554",
   "metadata": {},
   "source": [
    "## 1. Runtime and Repository Setup\n",
    "\n",
    "Use a **GPU runtime** before running this notebook (H100 preferred, T4 supported).\n",
    "\n",
    "If you cloned with an older commit, restart runtime and rerun from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!rm -rf lovli\n",
    "!git clone https://github.com/AndreasRamsli/lovli.git\n",
    "%cd /content/lovli\n",
    "\n",
    "# Install project with dependencies required by validation scripts.\n",
    "%pip install -q -U pip\n",
    "%pip install -q -e .\n",
    "\n",
    "# Safety net for environments where editable install path is delayed.\n",
    "import sys\n",
    "from pathlib import Path\n",
    "src_path = str(Path('/content/lovli/src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "print('Setup complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f'GPU: {name}')\n",
    "    print(f'VRAM: {props.total_memory / (1024**3):.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b803b9c",
   "metadata": {},
   "source": [
    "## 2. Environment Configuration (v3 + routing/coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "# Required Qdrant settings\n",
    "os.environ['QDRANT_URL'] = 'https://acc5c492-7d2c-4b95-b0c5-2931ff2ecebd.eu-west-1-0.aws.cloud.qdrant.io'\n",
    "os.environ['QDRANT_API_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.mpjUx25tLCPdDaEY31eqC1YcVBQijf2Tl4xru1F_56k'\n",
    "os.environ['QDRANT_COLLECTION_NAME'] = 'lovli_laws_v3'\n",
    "\n",
    "# Required by Settings model even for retrieval/eval scripts.\n",
    "os.environ['OPENROUTER_API_KEY'] = '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'\n",
    "\n",
    "# Keep traces off for speed/clean logs.\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "os.environ['LANGSMITH_TRACING'] = 'false'\n",
    "os.environ['SWEEP_SKIP_INDEX_SCAN'] = 'true'\n",
    "\n",
    "# Versioned trust profiles: switch TRUST_PROFILE between balanced_v1 and strict_v1.\n",
    "os.environ['TRUST_PROFILE_VERSION'] = '2026-02-16'\n",
    "profile_name = os.environ.get('TRUST_PROFILE', 'balanced_v1')\n",
    "profiles = {\n",
    "    'balanced_v1': {\n",
    "        'RETRIEVAL_K_INITIAL': '20',\n",
    "        'RERANKER_CONFIDENCE_THRESHOLD': '0.35',\n",
    "        'RERANKER_MIN_DOC_SCORE': '0.35',\n",
    "        'RERANKER_AMBIGUITY_MIN_GAP': '0.05',\n",
    "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
    "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'true',\n",
    "    },\n",
    "    'strict_v1': {\n",
    "        'RETRIEVAL_K_INITIAL': '15',\n",
    "        'RERANKER_CONFIDENCE_THRESHOLD': '0.45',\n",
    "        'RERANKER_MIN_DOC_SCORE': '0.55',\n",
    "        'RERANKER_AMBIGUITY_MIN_GAP': '0.10',\n",
    "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
    "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'false',\n",
    "    },\n",
    "}\n",
    "profile = profiles.get(profile_name, profiles['balanced_v1'])\n",
    "os.environ['TRUST_PROFILE'] = profile_name if profile_name in profiles else 'balanced_v1'\n",
    "\n",
    "# Shared law routing and coherence settings.\n",
    "os.environ['LAW_ROUTING_ENABLED'] = 'true'\n",
    "os.environ['LAW_CATALOG_PATH'] = 'data/law_catalog.json'\n",
    "os.environ['LAW_ROUTING_PREFILTER_K'] = '80'\n",
    "os.environ['LAW_ROUTING_RERANK_TOP_K'] = '6'\n",
    "os.environ['LAW_ROUTING_MIN_CONFIDENCE'] = '0.30'\n",
    "os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'] = '0.55'\n",
    "os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'] = '0.04'\n",
    "os.environ['LAW_ROUTING_FALLBACK_MAX_LAWS'] = '12'\n",
    "os.environ['LAW_COHERENCE_FILTER_ENABLED'] = 'true'\n",
    "os.environ['LAW_COHERENCE_MIN_LAW_COUNT'] = '2'\n",
    "os.environ['LAW_COHERENCE_SCORE_GAP'] = '0.15'\n",
    "os.environ['LAW_COHERENCE_RELATIVE_GAP'] = '0.05'\n",
    "os.environ['LAW_COHERENCE_MAX_SCORE_WEIGHT'] = '0.6'\n",
    "os.environ['LAW_COHERENCE_MIN_KEEP'] = '1'\n",
    "os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'] = '0.60'\n",
    "\n",
    "# Apply selected profile values.\n",
    "for key, value in profile.items():\n",
    "    os.environ[key] = value\n",
    "\n",
    "# Guard against accidental string values like 'None'.\n",
    "raw = os.environ.get('SWEEP_SAMPLE_SIZE')\n",
    "if raw is not None and raw.strip().lower() in {'', 'none', 'null'}:\n",
    "    os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
    "\n",
    "print('TRUST_PROFILE          =', os.environ['TRUST_PROFILE'])\n",
    "print('TRUST_PROFILE_VERSION  =', os.environ['TRUST_PROFILE_VERSION'])\n",
    "print('QDRANT_COLLECTION_NAME =', os.environ['QDRANT_COLLECTION_NAME'])\n",
    "print('LAW_ROUTING_ENABLED    =', os.environ['LAW_ROUTING_ENABLED'])\n",
    "print('LAW_CATALOG_PATH       =', os.environ['LAW_CATALOG_PATH'])\n",
    "print('LAW_ROUTING_PREFILTER  =', os.environ['LAW_ROUTING_PREFILTER_K'])\n",
    "print('LAW_ROUTING_RERANK_K   =', os.environ['LAW_ROUTING_RERANK_TOP_K'])\n",
    "print('LAW_ROUTING_CONF_MIN   =', os.environ['LAW_ROUTING_MIN_CONFIDENCE'])\n",
    "print('LAW_ROUTE_UNCERT_CEIL  =', os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'])\n",
    "print('LAW_ROUTE_UNCERT_GAP   =', os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'])\n",
    "print('LAW_ROUTE_FALLBACK     =', os.environ['LAW_ROUTING_FALLBACK_UNFILTERED'])\n",
    "print('LAW_COHERENCE_FILTER   =', os.environ['LAW_COHERENCE_FILTER_ENABLED'])\n",
    "print('LAW_COHERENCE_CONC_THR =', os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'])\n",
    "print('SWEEP_SAMPLE_SIZE      =', os.environ.get('SWEEP_SAMPLE_SIZE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional quick mode before full run.\n",
    "# Uncomment to run a small sample first.\n",
    "# os.environ['SWEEP_SAMPLE_SIZE'] = '100'\n",
    "\n",
    "# Ensure full run by default.\n",
    "os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
    "print('SWEEP_SAMPLE_SIZE now:', os.environ.get('SWEEP_SAMPLE_SIZE'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preflight: create run envelope and clear stale artifacts/logs.\n",
    "%cd /content/lovli\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import os\n",
    "\n",
    "run_id = datetime.now(timezone.utc).strftime('colab_%Y%m%dT%H%M%SZ')\n",
    "os.environ['LOVLI_RUN_ID'] = run_id\n",
    "print('LOVLI_RUN_ID =', run_id)\n",
    "\n",
    "preflight_targets = [\n",
    "    Path('eval/law_contamination_report.json'),\n",
    "    Path('eval/retrieval_sweep_results.json'),\n",
    "    Path('eval/logs/analyze_law_contamination.log'),\n",
    "    Path('eval/logs/retrieval_sweep_quick.log'),\n",
    "    Path('eval/logs/retrieval_sweep_full.log'),\n",
    "    Path('eval/logs/regression_gates.log'),\n",
    "]\n",
    "for target in preflight_targets:\n",
    "    if target.exists():\n",
    "        target.unlink()\n",
    "        print('removed', target)\n",
    "    else:\n",
    "        print('missing', target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce068d4d",
   "metadata": {},
   "source": [
    "## 3. Mount Drive, Extract Data, Build Catalog, Validate Reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f478d940",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/lovli\n",
    "\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "import json\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def run_to_log(cmd: str, log_path: Path) -> int:\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(log_path, 'w', encoding='utf-8') as log_file:\n",
    "        proc = subprocess.run(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            cwd='/content/lovli',\n",
    "            stdout=log_file,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "        )\n",
    "    return proc.returncode\n",
    "\n",
    "\n",
    "def print_log_matches(log_path: Path, patterns: list[str], limit: int = 50) -> None:\n",
    "    if not log_path.exists():\n",
    "        print(f'log missing: {log_path}')\n",
    "        return\n",
    "    lines = log_path.read_text(encoding='utf-8', errors='ignore').splitlines()\n",
    "    kept = []\n",
    "    for line in lines:\n",
    "        if any(p in line for p in patterns):\n",
    "            kept.append(line)\n",
    "    print(f'--- {log_path.name} (key lines) ---')\n",
    "    for line in kept[-limit:]:\n",
    "        print(line)\n",
    "\n",
    "\n",
    "# Mount Drive for access to the compressed dataset.\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update this path if your tar is moved.\n",
    "tar_path = Path('/content/drive/MyDrive/Colab Notebooks/Lovli/data/lovli-data.tar.bz2')\n",
    "assert tar_path.exists(), f'Data tar not found: {tar_path}'\n",
    "\n",
    "# Extract into repo data/ folder (safe to rerun).\n",
    "subprocess.run(\"mkdir -p /content/lovli/data\", shell=True, check=True)\n",
    "subprocess.run(\n",
    "    \"tar -xjf '/content/drive/MyDrive/Colab Notebooks/Lovli/data/lovli-data.tar.bz2' -C /content/lovli --exclude='._*'\",\n",
    "    shell=True,\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "nl_count = len(list(Path('/content/lovli/data/nl').glob('*.xml')))\n",
    "sf_count = len(list(Path('/content/lovli/data/sf').glob('*.xml')))\n",
    "print({'nl_xml_files': nl_count, 'sf_xml_files': sf_count})\n",
    "assert nl_count > 0 and sf_count > 0, 'Expected both data/nl and data/sf to contain XML files.'\n",
    "\n",
    "# Build merged catalog used by law routing (no summaries for speed).\n",
    "build_log = Path('/content/lovli/eval/logs/build_catalog.log')\n",
    "rc = run_to_log(\n",
    "    'python scripts/build_catalog.py data/nl data/sf --no-summaries --output data/law_catalog.json',\n",
    "    build_log,\n",
    ")\n",
    "print('build_catalog exit_code =', rc)\n",
    "print_log_matches(\n",
    "    build_log,\n",
    "    patterns=['Catalog build complete', 'Laws cataloged', 'With summaries', 'Missing summaries', 'Output:', 'Time:'],\n",
    ")\n",
    "assert rc == 0, 'build_catalog failed; inspect eval/logs/build_catalog.log'\n",
    "\n",
    "# Validate metadata + retrieval smoke checks on v3 collection.\n",
    "validate_log = Path('/content/lovli/eval/logs/validate_reindex.log')\n",
    "rc = run_to_log(\n",
    "    'python scripts/validate_reindex.py --collection lovli_laws_v3 --with-smoke',\n",
    "    validate_log,\n",
    ")\n",
    "print('validate_reindex exit_code =', rc)\n",
    "print_log_matches(\n",
    "    validate_log,\n",
    "    patterns=['Collection:', 'total_points=', 'missing_doc_type=', 'smoke query=', 'Validation completed.'],\n",
    ")\n",
    "assert rc == 0, 'validate_reindex failed; inspect eval/logs/validate_reindex.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308b8d5c",
   "metadata": {},
   "source": [
    "## 4. Law Contamination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db4a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/lovli\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "contam_log = Path('/content/lovli/eval/logs/analyze_law_contamination.log')\n",
    "rc = run_to_log(\n",
    "    'python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json',\n",
    "    contam_log,\n",
    ")\n",
    "print('analyze_law_contamination exit_code =', rc)\n",
    "print_log_matches(\n",
    "    contam_log,\n",
    "    patterns=['Processed', 'Saved contamination report', 'Contamination rate='],\n",
    ")\n",
    "\n",
    "report_path = Path('/content/lovli/eval/law_contamination_report.json')\n",
    "if report_path.exists():\n",
    "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
    "    agg = report.get('aggregate', {})\n",
    "    print('--- contamination aggregate ---')\n",
    "    for key in [\n",
    "        'total_questions',\n",
    "        'contamination_rate',\n",
    "        'singleton_foreign_rate',\n",
    "        'unexpected_citation_rate',\n",
    "        'mean_foreign_score_gap',\n",
    "    ]:\n",
    "        print(f'{key}: {agg.get(key)}')\n",
    "\n",
    "assert rc == 0, 'analyze_law_contamination failed; inspect eval/logs/analyze_law_contamination.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09926968",
   "metadata": {},
   "source": [
    "## 5. Full Retrieval Sweep (Colab run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/lovli\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "full_sweep_log = Path('/content/lovli/eval/logs/retrieval_sweep_full.log')\n",
    "rc = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', full_sweep_log)\n",
    "\n",
    "print('full sweep exit_code =', rc)\n",
    "print_log_matches(\n",
    "    full_sweep_log,\n",
    "    patterns=['Saved results:', 'Top 5 configurations:'],\n",
    ")\n",
    "\n",
    "sweep_path = Path('/content/lovli/eval/retrieval_sweep_results.json')\n",
    "if sweep_path.exists():\n",
    "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
    "    if rows:\n",
    "        top = rows[0]\n",
    "        print('--- full sweep top row ---')\n",
    "        for key in [\n",
    "            'is_profile_default_row',\n",
    "            'recall_at_k',\n",
    "            'citation_precision',\n",
    "            'unexpected_citation_rate',\n",
    "            'law_contamination_rate',\n",
    "            'law_coherence_filtered_count',\n",
    "            'balanced_score',\n",
    "        ]:\n",
    "            print(f'{key}: {top.get(key)}')\n",
    "\n",
    "assert rc == 0, 'full sweep failed; inspect eval/logs/retrieval_sweep_full.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6420a064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifact metadata compatibility check before regression gates.\n",
    "%cd /content/lovli\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "report_path = Path('eval/law_contamination_report.json')\n",
    "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
    "\n",
    "assert report_path.exists(), 'Missing contamination report artifact'\n",
    "assert sweep_path.exists(), 'Missing sweep results artifact'\n",
    "\n",
    "report = json.loads(report_path.read_text(encoding='utf-8'))\n",
    "rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
    "assert rows, 'Sweep results are empty'\n",
    "\n",
    "contam_meta = report.get('artifact_metadata', {})\n",
    "sweep_meta = {\n",
    "    'run_id': rows[0].get('run_id'),\n",
    "    'git_commit': rows[0].get('git_commit'),\n",
    "    'questions_sha256': rows[0].get('questions_sha256'),\n",
    "    'question_count': rows[0].get('question_count'),\n",
    "}\n",
    "\n",
    "print('contamination metadata:', contam_meta)\n",
    "print('sweep metadata:', sweep_meta)\n",
    "\n",
    "for key in ['run_id', 'git_commit', 'questions_sha256', 'question_count']:\n",
    "    left = contam_meta.get(key)\n",
    "    right = sweep_meta.get(key)\n",
    "    if left is None or right is None:\n",
    "        print(f'skip metadata check for {key}: value missing')\n",
    "        continue\n",
    "    assert str(left) == str(right), (\n",
    "        f'Artifact mismatch for {key}: contamination={left} sweep={right}'\n",
    "    )\n",
    "\n",
    "print('Artifact metadata compatibility check passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b098501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptance checks + focused debug for fallback-stage route misses.\n",
    "%cd /content/lovli\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "contam_path = Path('eval/law_contamination_report.json')\n",
    "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
    "assert contam_path.exists(), 'Missing eval/law_contamination_report.json'\n",
    "assert sweep_path.exists(), 'Missing eval/retrieval_sweep_results.json'\n",
    "\n",
    "contam = json.loads(contam_path.read_text(encoding='utf-8'))\n",
    "rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
    "assert rows, 'Sweep results are empty'\n",
    "\n",
    "profile_name = (contam.get('trust_profile_name') or '').strip()\n",
    "profile_rows = [r for r in rows if r.get('trust_profile_name') == profile_name] if profile_name else rows\n",
    "assert profile_rows, f'No sweep rows found for profile={profile_name!r}'\n",
    "default_rows = [r for r in profile_rows if bool(r.get('is_profile_default_row'))]\n",
    "assert len(default_rows) == 1, f'Expected one profile default row, found {len(default_rows)}'\n",
    "selected = default_rows[0]\n",
    "\n",
    "agg = contam.get('aggregate', {})\n",
    "for required_key in [\n",
    "    'fallback_stage_counts',\n",
    "    'route_miss_by_fallback_stage',\n",
    "    'route_miss_rate_by_stage',\n",
    "    'fallback_recovery_rate_by_stage',\n",
    "    'route_miss_count_by_mode_stage',\n",
    "]:\n",
    "    assert required_key in agg, f'Missing aggregate metric: {required_key}'\n",
    "\n",
    "print('=== Gate-selected default row ===')\n",
    "for k in [\n",
    "    'is_profile_default_row',\n",
    "    'retrieval_k_initial',\n",
    "    'retrieval_k',\n",
    "    'reranker_confidence_threshold',\n",
    "    'reranker_min_doc_score',\n",
    "    'law_routing_fallback_unfiltered',\n",
    "    'recall_at_k',\n",
    "    'citation_precision',\n",
    "    'unexpected_citation_rate',\n",
    "    'false_positive_gating_rate',\n",
    "    'balanced_score',\n",
    "    'routing_uncertain_count',\n",
    "    'fallback_stage1_accepted_count',\n",
    "    'fallback_stage2_unfiltered_count',\n",
    "]:\n",
    "    print(f'{k}: {selected.get(k)}')\n",
    "\n",
    "print('\\n=== Route miss by fallback stage ===')\n",
    "print('counts:', agg.get('route_miss_by_fallback_stage'))\n",
    "print('rates :', agg.get('route_miss_rate_by_stage'))\n",
    "\n",
    "print('\\n=== Fallback recovery by stage ===')\n",
    "print(agg.get('fallback_recovery_rate_by_stage'))\n",
    "\n",
    "print('\\n=== Top route-miss law pair confusions ===')\n",
    "for row in (agg.get('top_route_miss_law_pair_confusions') or [])[:10]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bacf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlled rerun envelope: contamination -> sweep -> gates (v1,v2).\n",
    "%cd /content/lovli\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "run_id = os.environ.get('LOVLI_RUN_ID') or datetime.now(timezone.utc).strftime('colab_%Y%m%dT%H%M%SZ')\n",
    "os.environ['LOVLI_RUN_ID'] = run_id\n",
    "profile_for_gates = (os.environ.get('TRUST_PROFILE') or 'balanced_v1').strip() or 'balanced_v1'\n",
    "os.environ['TRUST_PROFILE'] = profile_for_gates\n",
    "print('Controlled run envelope:', run_id)\n",
    "print('Gate profile:', profile_for_gates)\n",
    "\n",
    "logs_dir = Path('eval/logs')\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "contam_log = logs_dir / 'analyze_law_contamination.log'\n",
    "sweep_log = logs_dir / 'retrieval_sweep_full.log'\n",
    "gates_v1_log = logs_dir / 'regression_gates_v1.log'\n",
    "gates_v2_log = logs_dir / 'regression_gates_v2.log'\n",
    "\n",
    "base_gate_cmd = (\n",
    "    'python -u scripts/check_regression_gates.py '\n",
    "    '--contamination-report eval/law_contamination_report.json '\n",
    "    '--sweep-results eval/retrieval_sweep_results.json '\n",
    "    '--baseline eval/baselines/production_trust_baseline_v1.json '\n",
    "    f'--profile \"{profile_for_gates}\" '\n",
    ")\n",
    "\n",
    "rc_contam = run_to_log(\n",
    "    'python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json',\n",
    "    contam_log,\n",
    ")\n",
    "rc_sweep = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', sweep_log)\n",
    "rc_v1 = run_to_log(base_gate_cmd + '--gate-tier v1', gates_v1_log)\n",
    "rc_v2 = run_to_log(base_gate_cmd + '--gate-tier v2', gates_v2_log)\n",
    "\n",
    "print('exit codes:', {\n",
    "    'contamination': rc_contam,\n",
    "    'sweep': rc_sweep,\n",
    "    'gates_v1': rc_v1,\n",
    "    'gates_v2': rc_v2,\n",
    "})\n",
    "\n",
    "for path in [contam_log, sweep_log, gates_v1_log, gates_v2_log]:\n",
    "    assert path.exists(), f'Missing log: {path}'\n",
    "\n",
    "print_log_matches(contam_log, patterns=['Run metadata:', 'Saved contamination report', 'Gate summary:'])\n",
    "print_log_matches(sweep_log, patterns=['Run metadata:', 'Parity debug:', 'Parity divergence counts:', 'Saved results:'])\n",
    "print_log_matches(gates_v1_log, patterns=['Artifact metadata:', 'Gate checks passed', 'Regression gates failed', 'All regression gates passed'])\n",
    "print_log_matches(gates_v2_log, patterns=['Artifact metadata:', 'Gate checks passed', 'Regression gates failed', 'All regression gates passed'])\n",
    "\n",
    "report = json.loads(Path('eval/law_contamination_report.json').read_text(encoding='utf-8'))\n",
    "rows = json.loads(Path('eval/retrieval_sweep_results.json').read_text(encoding='utf-8'))\n",
    "assert rows, 'Sweep results are empty'\n",
    "profile = report.get('trust_profile_name')\n",
    "default_rows = [r for r in rows if r.get('trust_profile_name') == profile and bool(r.get('is_profile_default_row'))]\n",
    "assert len(default_rows) == 1, f'Expected one default row for profile={profile}, got {len(default_rows)}'\n",
    "default_row = default_rows[0]\n",
    "\n",
    "print('\\n=== Controlled run assessment (profile default row) ===')\n",
    "for key in ['recall_at_k', 'citation_precision', 'unexpected_citation_rate', 'false_positive_gating_rate', 'balanced_score', 'fallback_stage1_accepted_count', 'fallback_stage2_unfiltered_count']:\n",
    "    print(f'{key}: {default_row.get(key)}')\n",
    "\n",
    "agg = report.get('aggregate', {})\n",
    "for key in ['route_miss_expected_law_rate', 'dominant_law_mismatch_rate', 'fallback_recovery_rate', 'route_miss_by_fallback_stage', 'route_miss_rate_by_stage']:\n",
    "    print(f'{key}: {agg.get(key)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf086b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must-pass regression gates against versioned baseline.\n",
    "# Note: controlled rerun cell above already runs v1/v2; keep this as a standalone quick v1 check.\n",
    "%cd /content/lovli\n",
    "from pathlib import Path\n",
    "\n",
    "profile_for_gates = (os.environ.get('TRUST_PROFILE') or 'balanced_v1').strip() or 'balanced_v1'\n",
    "os.environ['TRUST_PROFILE'] = profile_for_gates\n",
    "print('Gate profile:', profile_for_gates)\n",
    "\n",
    "gates_log = Path('/content/lovli/eval/logs/regression_gates.log')\n",
    "rc = run_to_log(\n",
    "    'python -u scripts/check_regression_gates.py '\n",
    "    '--contamination-report eval/law_contamination_report.json '\n",
    "    '--sweep-results eval/retrieval_sweep_results.json '\n",
    "    '--baseline eval/baselines/production_trust_baseline_v1.json '\n",
    "    f'--profile \"{profile_for_gates}\" '\n",
    "    '--gate-tier v1',\n",
    "    gates_log,\n",
    ")\n",
    "print('regression gates exit_code =', rc)\n",
    "print_log_matches(\n",
    "    gates_log,\n",
    "    patterns=['Artifact metadata:', 'Gate sweep row selected', '[PASS]', '[FAIL]', 'Gate checks passed', 'All regression gates passed'],\n",
    "    limit=120,\n",
    ")\n",
    "assert rc == 0, 'regression gates failed; inspect eval/logs/regression_gates.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a061fef",
   "metadata": {},
   "source": [
    "## 6. Artifact Overview and Quick Metric Check\n",
    "\n",
    "Run acceptance targets (balanced objective):\n",
    "- `recall_at_k` should improve materially vs previous baseline (~0.146)\n",
    "- `citation_precision` should increase from previous baseline (~0.073)\n",
    "- `unexpected_citation_rate` should decrease\n",
    "- `law_coherence_filtered_count` should be non-zero on full sweep\n",
    "- `missing_doc_type` must remain `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1009eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/lovli\n",
    "!ls -lah eval\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "artifacts = [\n",
    "    Path('data/law_catalog.json'),\n",
    "    Path('eval/law_contamination_report.json'),\n",
    "    Path('eval/retrieval_sweep_results.json'),\n",
    "]\n",
    "for p in artifacts:\n",
    "    print(f'{p}:', 'exists' if p.exists() else 'missing')\n",
    "\n",
    "report_path = Path('eval/law_contamination_report.json')\n",
    "if report_path.exists():\n",
    "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
    "    agg = report.get('aggregate', {})\n",
    "    print('\\nContamination aggregate:')\n",
    "    for k in [\n",
    "        'total_questions',\n",
    "        'contamination_rate',\n",
    "        'singleton_foreign_rate',\n",
    "        'unexpected_citation_rate',\n",
    "        'mean_foreign_score_gap',\n",
    "    ]:\n",
    "        print(f'  {k}: {agg.get(k)}')\n",
    "\n",
    "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
    "if sweep_path.exists():\n",
    "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
    "    if rows:\n",
    "        top = rows[0]\n",
    "        print('\\nTop sweep row:')\n",
    "        for k in [\n",
    "            'recall_at_k',\n",
    "            'citation_precision',\n",
    "            'unexpected_citation_rate',\n",
    "            'law_contamination_rate',\n",
    "            'law_coherence_filtered_count',\n",
    "        ]:\n",
    "            print(f'  {k}: {top.get(k)}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "validate_reindex_h100_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}