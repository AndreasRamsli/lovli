{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lovli - Index + Source-Gating Validation (Colab A100 GPU)\n",
        "\n",
        "This notebook indexes Norwegian laws/regulations into Qdrant Cloud, then runs the v3 source-gating validation pipeline in Colab.\n",
        "\n",
        "**Requirements:**\n",
        "- Colab GPU runtime (A100 preferred)\n",
        "- `lovli-data.tar.bz2` in your Google Drive (root folder)\n",
        "- Qdrant Cloud URL and API key\n",
        "\n",
        "**Runs after indexing:**\n",
        "- `scripts/build_catalog.py` (merge `data/nl` + `data/sf`)\n",
        "- `scripts/validate_reindex.py`\n",
        "- `scripts/analyze_law_contamination.py`\n",
        "- `scripts/sweep_retrieval_thresholds.py`\n",
        "\n",
        "**Estimated time:** indexing ~20-30 min + evaluation/sweeps depending on GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "# Set HF_TOKEN if you have one (reduces rate limit warnings):\n",
        "# os.environ[\"HF_TOKEN\"] = \"your_token_here\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Colab bootstrap (robust)\n",
        "%cd /content\n",
        "!rm -rf lovli\n",
        "!git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Install base runtime deps first (avoid upgrading Colab's pinned requests)\n",
        "%pip install -q sentence-transformers qdrant-client beautifulsoup4\n",
        "\n",
        "# Install project in editable mode WITHOUT pulling/upgrading all deps again\n",
        "%pip install -q -e . --no-deps\n",
        "\n",
        "# Hard fallback: ensure src path is importable even if editable install is flaky\n",
        "import sys\n",
        "from pathlib import Path\n",
        "src_path = str(Path(\"/content/lovli/src\"))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Verify parser import\n",
        "import lovli.parser as lp\n",
        "print(\"Parser module:\", lp.__file__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    vram_gb = props.total_memory / (1024**3)\n",
        "    print(f\"GPU: {name} ({vram_gb:.1f} GB VRAM)\")\n",
        "    if \"A100\" not in name:\n",
        "        print(\"  Note: Optimized for A100; other GPUs may need smaller batch sizes\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > A100 GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Fill in your Qdrant Cloud credentials:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- FILL THESE IN ---\n",
        "QDRANT_URL = \"https://acc5c492-7d2c-4b95-b0c5-2931ff2ecebd.eu-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"\"  # Paste your Qdrant API key here, or use getpass below\n",
        "# ---------------------\n",
        "\n",
        "if not QDRANT_API_KEY:\n",
        "    import getpass\n",
        "    QDRANT_API_KEY = getpass.getpass(\"Qdrant API key: \")\n",
        "\n",
        "COLLECTION_NAME = \"lovli_laws_v3\"  # Blue/green: keep lovli_laws_v2 as rollback\n",
        "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "EMBEDDING_BATCH_SIZE = 256  # A100 80GB can handle large batches\n",
        "INDEX_BATCH_SIZE = 500      # Upsert batch size to Qdrant\n",
        "\n",
        "# Editorial payload guardrails\n",
        "EDITORIAL_NOTES_PER_PROVISION_CAP = 3\n",
        "EDITORIAL_NOTE_MAX_CHARS = 600\n",
        "\n",
        "# Network/retry tuning for Qdrant Cloud\n",
        "QDRANT_TIMEOUT_SECONDS = 120\n",
        "UPSERT_MAX_RETRIES = 5\n",
        "UPSERT_BACKOFF_SECONDS = 2\n",
        "\n",
        "# Runtime env for downstream validation scripts\n",
        "import os\n",
        "os.environ['QDRANT_URL'] = QDRANT_URL\n",
        "os.environ['QDRANT_API_KEY'] = QDRANT_API_KEY\n",
        "os.environ['QDRANT_COLLECTION_NAME'] = COLLECTION_NAME\n",
        "os.environ['OPENROUTER_API_KEY'] = os.environ.get('OPENROUTER_API_KEY', 'dummy')\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
        "os.environ['LANGSMITH_TRACING'] = 'false'\n",
        "os.environ['SWEEP_SKIP_INDEX_SCAN'] = 'true'\n",
        "\n",
        "# v3 retrieval profile\n",
        "os.environ['RETRIEVAL_K_INITIAL'] = '15'\n",
        "os.environ['RERANKER_CONFIDENCE_THRESHOLD'] = '0.45'\n",
        "os.environ['RERANKER_MIN_DOC_SCORE'] = '0.35'\n",
        "os.environ['RERANKER_AMBIGUITY_MIN_GAP'] = '0.05'\n",
        "os.environ['RERANKER_AMBIGUITY_TOP_SCORE_CEILING'] = '0.7'\n",
        "\n",
        "# law routing + coherence settings\n",
        "os.environ['LAW_ROUTING_ENABLED'] = 'true'\n",
        "os.environ['LAW_CATALOG_PATH'] = 'data/law_catalog.json'\n",
        "os.environ['LAW_COHERENCE_FILTER_ENABLED'] = 'true'\n",
        "os.environ['LAW_COHERENCE_MIN_LAW_COUNT'] = '2'\n",
        "os.environ['LAW_COHERENCE_SCORE_GAP'] = '0.15'\n",
        "\n",
        "assert QDRANT_API_KEY, \"Please set QDRANT_API_KEY above\"\n",
        "print('QDRANT_COLLECTION_NAME =', os.environ['QDRANT_COLLECTION_NAME'])\n",
        "print('LAW_ROUTING_ENABLED   =', os.environ['LAW_ROUTING_ENABLED'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data\n",
        "\n",
        "Mount Google Drive and extract `lovli-data.tar.bz2` directly from Drive (no copy step)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_DIRS = [\"data/nl\", \"data/sf\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract directly into the cloned repo (skip macOS ._ resource fork files).\n",
        "!tar -xjf /content/drive/MyDrive/lovli-data.tar.bz2 -C /content/lovli --exclude='._*'\n",
        "!ls /content/lovli/data/nl/*.xml 2>/dev/null | wc -l && ls /content/lovli/data/sf/*.xml 2>/dev/null | wc -l\n",
        "\n",
        "# Build merged law catalog used by routing (fast path: no summaries).\n",
        "%cd /content/lovli\n",
        "!python scripts/build_catalog.py data/nl data/sf --no-summaries --output data/law_catalog.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Optional Parser Sanity Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "from lovli.parser import parse_xml_file_grouped\n",
        "\n",
        "# Optional: quick parser sanity check before long indexing runs.\n",
        "sample = Path('data/nl/nl-19990326-017.xml')\n",
        "if sample.exists():\n",
        "    sample_articles = list(parse_xml_file_grouped(sample, per_provision_cap=3, editorial_note_max_chars=600))\n",
        "    assert sample_articles, 'Sample parse returned no provisions'\n",
        "    first = sample_articles[0]\n",
        "    print('Parser sanity OK:', {'count': len(sample_articles), 'first_article_id': first.article_id})\n",
        "else:\n",
        "    print('Skipping parser sanity check (sample file missing).')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# (Intentionally left minimal; parser sanity is covered in the previous cell.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# (Removed duplicate parser sanity logic to keep notebook concise.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Script-Based Indexing (recommended)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure runtime knobs consumed by scripts/index_laws.py (via Settings).\n",
        "import os\n",
        "\n",
        "os.environ['EMBEDDING_MODEL_NAME'] = EMBEDDING_MODEL\n",
        "os.environ['EMBEDDING_DIMENSION'] = str(EMBEDDING_DIMENSION)\n",
        "os.environ['EMBEDDING_BATCH_SIZE'] = str(EMBEDDING_BATCH_SIZE)\n",
        "os.environ['INDEX_BATCH_SIZE'] = str(INDEX_BATCH_SIZE)\n",
        "os.environ['EDITORIAL_NOTES_PER_PROVISION_CAP'] = str(EDITORIAL_NOTES_PER_PROVISION_CAP)\n",
        "os.environ['EDITORIAL_NOTE_MAX_CHARS'] = str(EDITORIAL_NOTE_MAX_CHARS)\n",
        "\n",
        "print('Script indexing config:')\n",
        "print('  EMBEDDING_MODEL_NAME =', os.environ['EMBEDDING_MODEL_NAME'])\n",
        "print('  EMBEDDING_BATCH_SIZE =', os.environ['EMBEDDING_BATCH_SIZE'])\n",
        "print('  INDEX_BATCH_SIZE     =', os.environ['INDEX_BATCH_SIZE'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Index with scripts/index_laws.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "# Recreate collection and index first directory.\n",
        "# Then append second directory into the same collection.\n",
        "!python scripts/index_laws.py data/nl --collection lovli_laws_v3 --recreate\n",
        "!python scripts/index_laws.py data/sf --collection lovli_laws_v3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optional Retry Pass (if indexing failed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "# Optional rerun if one of the previous indexing commands failed.\n",
        "# Re-run only the failing directory.\n",
        "# !python scripts/index_laws.py data/nl --collection lovli_laws_v3\n",
        "# !python scripts/index_laws.py data/sf --collection lovli_laws_v3\n",
        "\n",
        "print('Indexing is script-driven now; use the commands above for retries.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Ensure Payload Indexes (idempotent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!python scripts/create_payload_indexes.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Optional Full Rebuild (if needed)\n",
        "\n",
        "Use this only if you want a fresh collection rebuild after major indexing changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "# Full rebuild flow (uncomment to run):\n",
        "# !python scripts/index_laws.py data/nl --collection lovli_laws_v3 --recreate\n",
        "# !python scripts/index_laws.py data/sf --collection lovli_laws_v3\n",
        "\n",
        "print('Uncomment both commands above only if you want a full rebuild.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Verify Collection and Run Source-Gating Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "# Scripted metadata + smoke validation (includes collection-level sanity output).\n",
        "!python scripts/validate_reindex.py --collection lovli_laws_v3 --with-smoke\n",
        "\n",
        "# Cross-law contamination analysis\n",
        "!python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## 11. Retrieval Sweep (quick check + full run)\n",
        "\n",
        "import os\n",
        "%cd /content/lovli\n",
        "\n",
        "# Quick check (optional but recommended before full run)\n",
        "os.environ['SWEEP_SAMPLE_SIZE'] = '10'\n",
        "!python -u scripts/sweep_retrieval_thresholds.py\n",
        "os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "\n",
        "# Full run\n",
        "!python -u scripts/sweep_retrieval_thresholds.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Artifact Overview and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!ls -lah eval\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts = [\n",
        "    Path('data/law_catalog.json'),\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "]\n",
        "for p in artifacts:\n",
        "    print(f'{p}:', 'exists' if p.exists() else 'missing')\n",
        "\n",
        "report_path = Path('eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('\\nContamination aggregate:')\n",
        "    for k in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'  {k}: {agg.get(k)}')\n",
        "\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('\\nTop sweep row:')\n",
        "        for k in [\n",
        "            'recall_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "        ]:\n",
        "            print(f'  {k}: {top.get(k)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}