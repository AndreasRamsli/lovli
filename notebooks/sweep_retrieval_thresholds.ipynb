{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lovli - Retrieval Threshold Sweep (Colab T4)\n",
        "\n",
        "This notebook runs a retrieval-quality sweep over threshold combinations for Lovli.\n",
        "\n",
        "It is designed for Google Colab with a T4 GPU to avoid local MPS memory pressure.\n",
        "\n",
        "## What it does\n",
        "- Loads the project and dependencies\n",
        "- Builds one `LegalRAGChain`\n",
        "- Precomputes retrieval/reranker candidates once\n",
        "- Sweeps combinations of:\n",
        "  - `retrieval_k_initial`\n",
        "  - `reranker_confidence_threshold`\n",
        "  - `reranker_min_doc_score`\n",
        "- Saves results to `eval/retrieval_sweep_results.json`\n",
        "\n",
        "## Expected outputs\n",
        "- Ranked sweep table in notebook output\n",
        "- JSON file with all combinations and metrics"
      ],
      "id": "3b00dedc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# If running in Colab, clone your repo first (skip if already cloned).\n",
        "import os\n",
        "if not os.path.exists(\"/content/lovli\"):\n",
        "    !git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Satisfy google-colab's requests pin to avoid dependency conflicts\n",
        "!pip install -q \"requests==2.32.4\"\n",
        "!pip install -q -e ."
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f8aac167"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "import itertools\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Reduce noisy tracing during local experiments\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "\n",
        "# Find repo root: Colab clone is /content/lovli; fallback to cwd if running from repo\n",
        "ROOT_DIR = Path(\"/content/lovli\")\n",
        "if not (ROOT_DIR / \"src\" / \"lovli\").exists():\n",
        "    for cand in [Path.cwd(), Path.cwd().parent]:\n",
        "        if (cand / \"src\" / \"lovli\").exists():\n",
        "            ROOT_DIR = cand\n",
        "            break\n",
        "if not (ROOT_DIR / \"src\" / \"lovli\").exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"lovli package not found. Run the setup cell above first (clone + pip install), \"\n",
        "        \"then Runtime > Restart runtime, then run from this cell.\"\n",
        "    )\n",
        "if str(ROOT_DIR / \"src\") not in sys.path:\n",
        "    sys.path.insert(0, str(ROOT_DIR / \"src\"))\n",
        "\n",
        "from lovli.chain import LegalRAGChain\n",
        "from lovli.config import get_settings\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2be9fbd1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sweep only uses retrieval/reranking (no LLM). Use placeholder so Settings loads.\n",
        "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
        "    os.environ[\"OPENROUTER_API_KEY\"] = \"sweep-placeholder-not-used\"\n",
        "\n",
        "# Qdrant is required. Set via .env in repo root, or uncomment to prompt:\n",
        "# import getpass\n",
        "# os.environ[\"QDRANT_URL\"] = input(\"QDRANT_URL (https://...): \").strip()\n",
        "# os.environ[\"QDRANT_API_KEY\"] = getpass.getpass(\"QDRANT_API_KEY: \").strip()\n",
        "\n",
        "QUESTIONS_PATH = ROOT_DIR / \"eval\" / \"questions.jsonl\"\n",
        "OUT_PATH = ROOT_DIR / \"eval\" / \"retrieval_sweep_results.json\"\n",
        "\n",
        "# Optional: set to an int like 20 for a quick dry run\n",
        "SAMPLE_SIZE = None"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "eff91d7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def matches_expected(cited_id: str, expected_set: set[str]) -> bool:\n",
        "    return any(cited_id.startswith(exp) for exp in expected_set)\n",
        "\n",
        "\n",
        "def load_questions(path: Path) -> list[dict]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "\n",
        "def precompute_candidates(chain: LegalRAGChain, questions: list[dict], max_k_initial: int) -> list[dict]:\n",
        "    chain.retriever = chain.vectorstore.as_retriever(search_kwargs={\"k\": max_k_initial})\n",
        "    cached = []\n",
        "\n",
        "    for row in questions:\n",
        "        query = row[\"question\"]\n",
        "        docs = chain._invoke_retriever(query, routed_law_ids=chain._route_law_ids(query))\n",
        "\n",
        "        dedup_docs = []\n",
        "        seen_keys = set()\n",
        "        for doc in docs:\n",
        "            metadata = doc.metadata if hasattr(doc, \"metadata\") else doc.get(\"metadata\", {})\n",
        "            key = (metadata.get(\"law_id\"), metadata.get(\"article_id\"))\n",
        "            if metadata.get(\"article_id\") and key not in seen_keys:\n",
        "                seen_keys.add(key)\n",
        "                dedup_docs.append(doc)\n",
        "            elif not metadata.get(\"article_id\"):\n",
        "                dedup_docs.append(doc)\n",
        "        docs = dedup_docs[:max_k_initial]\n",
        "\n",
        "        candidates = []\n",
        "        if docs:\n",
        "            pairs = [[query, doc.page_content if hasattr(doc, \"page_content\") else str(doc)] for doc in docs]\n",
        "            raw_scores = chain.reranker.predict(pairs) if chain.reranker else [1.0] * len(docs)\n",
        "            if hasattr(raw_scores, \"tolist\"):\n",
        "                raw_scores = raw_scores.tolist()\n",
        "            scores = [float(s) for s in raw_scores]\n",
        "            normalized = [1.0 / (1.0 + math.exp(-s)) for s in scores]\n",
        "\n",
        "            for doc, score in zip(docs, normalized):\n",
        "                metadata = doc.metadata if hasattr(doc, \"metadata\") else doc.get(\"metadata\", {})\n",
        "                candidates.append({\n",
        "                    \"law_id\": metadata.get(\"law_id\", \"\"),\n",
        "                    \"article_id\": metadata.get(\"article_id\", \"\"),\n",
        "                    \"score\": score,\n",
        "                })\n",
        "\n",
        "        cached.append({\n",
        "            \"id\": row.get(\"id\"),\n",
        "            \"expected_articles\": row.get(\"expected_articles\", []),\n",
        "            \"candidates\": candidates,\n",
        "        })\n",
        "\n",
        "    return cached\n",
        "\n",
        "\n",
        "def evaluate_combo(chain: LegalRAGChain, cached_candidates: list[dict], retrieval_k_initial: int, confidence_threshold: float, min_doc_score: float) -> dict:\n",
        "    retrieval_hits = 0\n",
        "    retrieval_total = 0\n",
        "    ambiguity_total = 0\n",
        "    ambiguity_clean = 0\n",
        "    top_scores = []\n",
        "\n",
        "    final_k = chain.settings.retrieval_k\n",
        "    min_sources = chain.settings.reranker_min_sources\n",
        "\n",
        "    for row in cached_candidates:\n",
        "        expected = set(row.get(\"expected_articles\", []))\n",
        "        candidates = row.get(\"candidates\", [])\n",
        "\n",
        "        subset = candidates[:retrieval_k_initial]\n",
        "        ranked = sorted(subset, key=lambda x: x[\"score\"], reverse=True)[:final_k]\n",
        "\n",
        "        kept = [c for c in ranked if c[\"score\"] >= min_doc_score]\n",
        "        if len(kept) < min(min_sources, len(ranked)):\n",
        "            kept = ranked[: min(min_sources, len(ranked))]\n",
        "\n",
        "        scores = [c[\"score\"] for c in kept]\n",
        "        cited_ids = [c[\"article_id\"] for c in kept if c.get(\"article_id\")]\n",
        "        top_score = scores[0] if scores else None\n",
        "\n",
        "        if top_score is not None:\n",
        "            top_scores.append(top_score)\n",
        "\n",
        "        if expected:\n",
        "            retrieval_total += 1\n",
        "            if any(matches_expected(cid, expected) for cid in cited_ids):\n",
        "                retrieval_hits += 1\n",
        "        else:\n",
        "            ambiguity_total += 1\n",
        "            is_gated = False\n",
        "            if top_score is not None and top_score < confidence_threshold:\n",
        "                is_gated = True\n",
        "            if (\n",
        "                not is_gated\n",
        "                and chain.settings.reranker_ambiguity_gating_enabled\n",
        "                and len(scores) >= 2\n",
        "                and scores[0] <= chain.settings.reranker_ambiguity_top_score_ceiling\n",
        "                and (scores[0] - scores[1]) < chain.settings.reranker_ambiguity_min_gap\n",
        "            ):\n",
        "                is_gated = True\n",
        "            if is_gated or not cited_ids:\n",
        "                ambiguity_clean += 1\n",
        "\n",
        "    recall_at_k = retrieval_hits / retrieval_total if retrieval_total else 0.0\n",
        "    ambiguity_success = ambiguity_clean / ambiguity_total if ambiguity_total else 1.0\n",
        "    avg_top_score = sum(top_scores) / len(top_scores) if top_scores else 0.0\n",
        "\n",
        "    return {\n",
        "        \"recall_at_k\": recall_at_k,\n",
        "        \"ambiguity_success\": ambiguity_success,\n",
        "        \"avg_top_score\": avg_top_score,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "80adb690"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "questions = load_questions(QUESTIONS_PATH)\n",
        "if SAMPLE_SIZE:\n",
        "    questions = questions[:SAMPLE_SIZE]\n",
        "\n",
        "settings = get_settings()\n",
        "chain = LegalRAGChain(settings=settings)\n",
        "\n",
        "# Fixed baseline from previous sweep\n",
        "BASE_RETRIEVAL_K_INITIAL = 15\n",
        "BASE_CONFIDENCE = 0.4\n",
        "BASE_MIN_DOC_SCORE = 0.3\n",
        "\n",
        "# Ambiguity-focused grid\n",
        "MIN_GAP_VALUES = [0.05, 0.08, 0.10, 0.12, 0.15]\n",
        "TOP_SCORE_CEILING_VALUES = [0.55, 0.60, 0.65, 0.70]\n",
        "\n",
        "max_k_initial = BASE_RETRIEVAL_K_INITIAL\n",
        "logger.info(\"Precomputing candidates once with k=%s on %s questions...\", max_k_initial, len(questions))\n",
        "cached_candidates = precompute_candidates(chain, questions, max_k_initial=max_k_initial)\n",
        "\n",
        "rows = []\n",
        "for min_gap, top_score_ceiling in itertools.product(\n",
        "    MIN_GAP_VALUES,\n",
        "    TOP_SCORE_CEILING_VALUES,\n",
        "):\n",
        "    chain.settings.retrieval_k_initial = BASE_RETRIEVAL_K_INITIAL\n",
        "    chain.settings.reranker_confidence_threshold = BASE_CONFIDENCE\n",
        "    chain.settings.reranker_min_doc_score = BASE_MIN_DOC_SCORE\n",
        "    chain.settings.reranker_ambiguity_min_gap = min_gap\n",
        "    chain.settings.reranker_ambiguity_top_score_ceiling = top_score_ceiling\n",
        "\n",
        "    metrics = evaluate_combo(\n",
        "        chain,\n",
        "        cached_candidates,\n",
        "        retrieval_k_initial=BASE_RETRIEVAL_K_INITIAL,\n",
        "        confidence_threshold=BASE_CONFIDENCE,\n",
        "        min_doc_score=BASE_MIN_DOC_SCORE,\n",
        "    )\n",
        "    row = {\n",
        "        \"retrieval_k_initial\": BASE_RETRIEVAL_K_INITIAL,\n",
        "        \"reranker_confidence_threshold\": BASE_CONFIDENCE,\n",
        "        \"reranker_min_doc_score\": BASE_MIN_DOC_SCORE,\n",
        "        \"reranker_ambiguity_min_gap\": min_gap,\n",
        "        \"reranker_ambiguity_top_score_ceiling\": top_score_ceiling,\n",
        "        **metrics,\n",
        "    }\n",
        "    rows.append(row)\n",
        "\n",
        "rows.sort(key=lambda r: (r[\"ambiguity_success\"], r[\"recall_at_k\"]), reverse=True)\n",
        "\n",
        "with open(OUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(rows, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Saved: {OUT_PATH}\")\n",
        "print(\"Top 5 configurations:\")\n",
        "for row in rows[:5]:\n",
        "    print(row)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "552f8600"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- For a quick pass, set `SAMPLE_SIZE = 20` first.\n",
        "- For final tuning, set `SAMPLE_SIZE = None`.\n",
        "- If reranker/model loading still feels slow, keep this notebook on T4 and avoid running in local MPS.\n",
        "- You can copy the best row values into your `.env` or `Settings` defaults after validation."
      ],
      "id": "8d01b5c9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}