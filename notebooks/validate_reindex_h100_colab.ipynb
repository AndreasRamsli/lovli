{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lovli Source-Gating Validation Run (Colab GPU)\n",
        "\n",
        "This notebook runs the **v3 source-gating workflow** on Colab (H100/T4 compatible), so we avoid local RAM limits.\n",
        "\n",
        "It runs:\n",
        "- `scripts/build_catalog.py` (merge `data/nl` + `data/sf`)\n",
        "- `scripts/validate_reindex.py`\n",
        "- `scripts/analyze_law_contamination.py`\n",
        "- `scripts/sweep_retrieval_thresholds.py`\n",
        "\n",
        "The setup enables law routing + law coherence filtering, then exports analysis artifacts for review."
      ],
      "id": "d2ab62ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Runtime and Repository Setup\n",
        "\n",
        "Use a **GPU runtime** before running this notebook (H100 preferred, T4 supported).\n",
        "\n",
        "If you cloned with an older commit, restart runtime and rerun from the top."
      ],
      "id": "4f0df554"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content\n",
        "!rm -rf lovli\n",
        "!git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Install project with dependencies required by validation scripts.\n",
        "%pip install -q -U pip\n",
        "%pip install -q -e .\n",
        "\n",
        "# Safety net for environments where editable install path is delayed.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "src_path = str(Path('/content/lovli/src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "print('Setup complete')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f'GPU: {name}')\n",
        "    print(f'VRAM: {props.total_memory / (1024**3):.1f} GB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Environment Configuration (v3 + routing/coherence)"
      ],
      "id": "4b803b9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Required Qdrant settings\n",
        "os.environ['QDRANT_URL'] = 'https://acc5c492-7d2c-4b95-b0c5-2931ff2ecebd.eu-west-1-0.aws.cloud.qdrant.io'\n",
        "os.environ['QDRANT_API_KEY'] = getpass.getpass('Qdrant API key: ')\n",
        "os.environ['QDRANT_COLLECTION_NAME'] = 'lovli_laws_v3'\n",
        "\n",
        "# Required by Settings model even for retrieval/eval scripts.\n",
        "os.environ['OPENROUTER_API_KEY'] = os.environ.get('OPENROUTER_API_KEY', 'dummy')\n",
        "\n",
        "# Keep traces off for speed/clean logs.\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
        "os.environ['LANGSMITH_TRACING'] = 'false'\n",
        "os.environ['SWEEP_SKIP_INDEX_SCAN'] = 'true'\n",
        "\n",
        "# v3 retrieval profile\n",
        "os.environ['RETRIEVAL_K_INITIAL'] = '15'\n",
        "os.environ['RERANKER_CONFIDENCE_THRESHOLD'] = '0.45'\n",
        "os.environ['RERANKER_MIN_DOC_SCORE'] = '0.35'\n",
        "os.environ['RERANKER_AMBIGUITY_MIN_GAP'] = '0.05'\n",
        "os.environ['RERANKER_AMBIGUITY_TOP_SCORE_CEILING'] = '0.7'\n",
        "\n",
        "# Enable law routing and law coherence filtering\n",
        "os.environ['LAW_ROUTING_ENABLED'] = 'true'\n",
        "os.environ['LAW_CATALOG_PATH'] = 'data/law_catalog.json'\n",
        "os.environ['LAW_ROUTING_PREFILTER_K'] = '80'\n",
        "os.environ['LAW_ROUTING_RERANK_TOP_K'] = '6'\n",
        "os.environ['LAW_ROUTING_MIN_CONFIDENCE'] = '0.30'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'] = '0.55'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'] = '0.04'\n",
        "os.environ['LAW_ROUTING_FALLBACK_UNFILTERED'] = 'true'\n",
        "os.environ['LAW_ROUTING_FALLBACK_MAX_LAWS'] = '12'\n",
        "os.environ['LAW_COHERENCE_FILTER_ENABLED'] = 'true'\n",
        "os.environ['LAW_COHERENCE_MIN_LAW_COUNT'] = '2'\n",
        "os.environ['LAW_COHERENCE_SCORE_GAP'] = '0.15'\n",
        "os.environ['LAW_COHERENCE_RELATIVE_GAP'] = '0.05'\n",
        "os.environ['LAW_COHERENCE_MAX_SCORE_WEIGHT'] = '0.6'\n",
        "os.environ['LAW_COHERENCE_MIN_KEEP'] = '1'\n",
        "\n",
        "# Guard against accidental string values like 'None'.\n",
        "raw = os.environ.get('SWEEP_SAMPLE_SIZE')\n",
        "if raw is not None and raw.strip().lower() in {'', 'none', 'null'}:\n",
        "    os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "\n",
        "print('QDRANT_COLLECTION_NAME =', os.environ['QDRANT_COLLECTION_NAME'])\n",
        "print('LAW_ROUTING_ENABLED   =', os.environ['LAW_ROUTING_ENABLED'])\n",
        "print('LAW_CATALOG_PATH      =', os.environ['LAW_CATALOG_PATH'])\n",
        "print('LAW_ROUTING_PREFILTER =', os.environ['LAW_ROUTING_PREFILTER_K'])\n",
        "print('LAW_ROUTING_RERANK_K  =', os.environ['LAW_ROUTING_RERANK_TOP_K'])\n",
        "print('LAW_ROUTING_CONF_MIN  =', os.environ['LAW_ROUTING_MIN_CONFIDENCE'])\n",
        "print('LAW_ROUTE_UNCERT_CEIL =', os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'])\n",
        "print('LAW_ROUTE_UNCERT_GAP  =', os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'])\n",
        "print('LAW_ROUTE_FALLBACK    =', os.environ['LAW_ROUTING_FALLBACK_UNFILTERED'])\n",
        "print('LAW_COHERENCE_FILTER  =', os.environ['LAW_COHERENCE_FILTER_ENABLED'])\n",
        "print('SWEEP_SAMPLE_SIZE     =', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ce9f311d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional quick mode before full run.\n",
        "# Uncomment to run a small sample first.\n",
        "# os.environ['SWEEP_SAMPLE_SIZE'] = '100'\n",
        "\n",
        "# Ensure full run by default.\n",
        "os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "print('SWEEP_SAMPLE_SIZE now:', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mount Drive, Extract Data, Build Catalog, Validate Reindex"
      ],
      "id": "ce068d4d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive for access to the compressed dataset.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Update this path if your tar is moved.\n",
        "tar_path = Path('/content/drive/MyDrive/Colab notebooks/Lovli/data/lovli-data.tar.bz2')\n",
        "assert tar_path.exists(), f'Data tar not found: {tar_path}'\n",
        "\n",
        "# Extract into repo data/ folder (safe to rerun).\n",
        "!mkdir -p /content/lovli/data\n",
        "!tar -xjf \"/content/drive/MyDrive/Colab notebooks/Lovli/data/lovli-data.tar.bz2\" -C /content/lovli --exclude='._*'\n",
        "\n",
        "nl_count = len(list(Path('/content/lovli/data/nl').glob('*.xml')))\n",
        "sf_count = len(list(Path('/content/lovli/data/sf').glob('*.xml')))\n",
        "print({'nl_xml_files': nl_count, 'sf_xml_files': sf_count})\n",
        "assert nl_count > 0 and sf_count > 0, 'Expected both data/nl and data/sf to contain XML files.'\n",
        "\n",
        "# Build merged catalog used by law routing (no summaries for speed).\n",
        "!python scripts/build_catalog.py data/nl data/sf --no-summaries --output data/law_catalog.json\n",
        "\n",
        "# Validate metadata + retrieval smoke checks on v3 collection.\n",
        "!python scripts/validate_reindex.py --collection lovli_laws_v3 --with-smoke"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f478d940"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Law Contamination Analysis"
      ],
      "id": "308b8d5c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6db4a328"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Optional quick check before full sweep.\n",
        "# Use a small sample to verify config quickly.\n",
        "%cd /content/lovli\n",
        "os.environ['SWEEP_SAMPLE_SIZE'] = '10'\n",
        "!python -u scripts/sweep_retrieval_thresholds.py\n",
        "os.environ.pop('SWEEP_SAMPLE_SIZE', None)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a30a858c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Full Retrieval Sweep (Colab run)"
      ],
      "id": "09926968"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!python -u scripts/sweep_retrieval_thresholds.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Artifact Overview and Quick Metric Check\n",
        "\n",
        "Run acceptance targets (balanced objective):\n",
        "- `recall_at_k` should improve materially vs previous baseline (~0.146)\n",
        "- `citation_precision` should increase from previous baseline (~0.073)\n",
        "- `unexpected_citation_rate` should decrease\n",
        "- `law_coherence_filtered_count` should be non-zero on full sweep\n",
        "- `missing_doc_type` must remain `0`"
      ],
      "id": "1a061fef"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%cd /content/lovli\n",
        "!ls -lah eval\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts = [\n",
        "    Path('data/law_catalog.json'),\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "]\n",
        "for p in artifacts:\n",
        "    print(f'{p}:', 'exists' if p.exists() else 'missing')\n",
        "\n",
        "report_path = Path('eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('\\nContamination aggregate:')\n",
        "    for k in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'  {k}: {agg.get(k)}')\n",
        "\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('\\nTop sweep row:')\n",
        "        for k in [\n",
        "            'recall_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "        ]:\n",
        "            print(f'  {k}: {top.get(k)}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9d1009eb"
    }
  ],
  "metadata": {
    "colab": {
      "name": "validate_reindex_h100_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}