{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lovli - Index Laws to Qdrant (Colab A100 GPU)\n",
        "\n",
        "This notebook indexes all Norwegian laws and regulations into Qdrant Cloud using an A100 GPU for fast embedding generation.\n",
        "\n",
        "**Requirements:**\n",
        "- Colab A100 GPU runtime (Runtime > Change runtime type > A100)\n",
        "- `lovli-data.tar.bz2` in your Google Drive (root folder)\n",
        "- Qdrant Cloud URL and API key\n",
        "\n",
        "**Estimated time:** ~20-30 minutes for ~4,000 files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "# Set HF_TOKEN if you have one (reduces rate limit warnings):\n",
        "# os.environ[\"HF_TOKEN\"] = \"your_token_here\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q sentence-transformers qdrant-client beautifulsoup4\n",
        "# Clone repo into Colab runtime (fresh session)\n",
        "%cd /content\n",
        "!rm -rf lovli\n",
        "!git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Install project package so `from lovli.parser import ...` works\n",
        "!pip install -q -e .\n",
        "\n",
        "# Optional: verify parser source\n",
        "import lovli.parser as lp\n",
        "print(lp.__file__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    vram_gb = props.total_memory / (1024**3)\n",
        "    print(f\"GPU: {name} ({vram_gb:.1f} GB VRAM)\")\n",
        "    if \"A100\" not in name:\n",
        "        print(\"  Note: Optimized for A100; other GPUs may need smaller batch sizes\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Go to Runtime > Change runtime type > A100 GPU\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Fill in your Qdrant Cloud credentials:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# --- FILL THESE IN ---\n",
        "QDRANT_URL = \"https://acc5c492-7d2c-4b95-b0c5-2931ff2ecebd.eu-west-1-0.aws.cloud.qdrant.io\"\n",
        "QDRANT_API_KEY = \"\"  # Paste your Qdrant API key here, or use getpass below\n",
        "# ---------------------\n",
        "\n",
        "if not QDRANT_API_KEY:\n",
        "    import getpass\n",
        "    QDRANT_API_KEY = getpass.getpass(\"Qdrant API key: \")\n",
        "\n",
        "COLLECTION_NAME = \"lovli_laws\"\n",
        "EMBEDDING_MODEL = \"BAAI/bge-m3\"\n",
        "EMBEDDING_DIMENSION = 1024\n",
        "EMBEDDING_BATCH_SIZE = 256  # A100 80GB can handle large batches\n",
        "INDEX_BATCH_SIZE = 500      # Upsert batch size to Qdrant\n",
        "\n",
        "# Network/retry tuning for Qdrant Cloud\n",
        "QDRANT_TIMEOUT_SECONDS = 120\n",
        "UPSERT_MAX_RETRIES = 5\n",
        "UPSERT_BACKOFF_SECONDS = 2\n",
        "\n",
        "assert QDRANT_API_KEY, \"Please set QDRANT_API_KEY above\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data\n",
        "\n",
        "Mount Google Drive and extract `lovli-data.tar.bz2` directly from Drive (no copy step)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_DIRS = [\"data/nl\", \"data/sf\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract directly into the cloned repo (skip macOS ._ resource fork files).\n",
        "!tar -xjf /content/drive/MyDrive/lovli-data.tar.bz2 -C /content/lovli --exclude='._*'\n",
        "!ls /content/lovli/data/nl/*.xml 2>/dev/null | wc -l && ls /content/lovli/data/sf/*.xml 2>/dev/null | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Parser (from lovli/parser.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import hashlib\n",
        "import logging\n",
        "import re\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from pathlib import Path\n",
        "from typing import Iterator\n",
        "\n",
        "from bs4 import BeautifulSoup, Tag\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "LOVDATA_BASE_URL = \"https://lovdata.no\"\n",
        "\n",
        "\n",
        "@dataclass(slots=True)\n",
        "class LegalArticle:\n",
        "    article_id: str\n",
        "    title: str\n",
        "    content: str\n",
        "    law_id: str\n",
        "    law_title: str\n",
        "    law_short_name: str | None = None\n",
        "    chapter_id: str | None = None\n",
        "    chapter_title: str | None = None\n",
        "    cross_references: list[str] = field(default_factory=list)\n",
        "    url: str | None = None\n",
        "\n",
        "\n",
        "def _extract_law_ref_from_filename(filename: str) -> str:\n",
        "    parts = filename.split(\"-\")\n",
        "    if len(parts) >= 3:\n",
        "        prefix = parts[0]\n",
        "        date_part = parts[1]\n",
        "        num_part = parts[2]\n",
        "        if len(date_part) == 8:\n",
        "            year, month, day = date_part[:4], date_part[4:6], date_part[6:8]\n",
        "            ref_prefix = \"forskrift\" if prefix == \"sf\" else \"lov\"\n",
        "            return f\"{ref_prefix}/{year}-{month}-{day}-{num_part}\"\n",
        "    return filename\n",
        "\n",
        "\n",
        "def _extract_short_name(soup: BeautifulSoup) -> str | None:\n",
        "    short_elem = soup.find(\"dd\", class_=\"titleShort\")\n",
        "    if not short_elem:\n",
        "        return None\n",
        "    text = short_elem.get_text(strip=True)\n",
        "    for sep in (\" \\u2013 \", \" \\u2014 \", \" - \"):\n",
        "        if sep in text:\n",
        "            return text.split(sep)[0].strip()\n",
        "    return text.strip() or None\n",
        "\n",
        "\n",
        "def _extract_cross_references(article_element: Tag, self_law_ref: str) -> list[str]:\n",
        "    refs, seen = [], set()\n",
        "    for a_tag in article_element.find_all(\"a\", href=True):\n",
        "        href = a_tag.get(\"href\", \"\")\n",
        "        if not href or not (href.startswith(\"lov/\") or href.startswith(\"forskrift/\")):\n",
        "            continue\n",
        "        if self_law_ref and href.startswith(self_law_ref):\n",
        "            continue\n",
        "        base_href = href.split(\"#\")[0] if \"#\" in href else href\n",
        "        if base_href not in seen:\n",
        "            seen.add(base_href)\n",
        "            refs.append(base_href)\n",
        "    return refs\n",
        "\n",
        "\n",
        "def _extract_article(article, idx, law_id, law_ref, law_title_text, law_short_name, chapter_id, chapter_title, xml_path):\n",
        "    try:\n",
        "        article_id = article.get(\"id\") or f\"{law_id}_art_{idx}\"\n",
        "        if \"-ledd-\" in article_id or \"-punkt-\" in article_id:\n",
        "            return None\n",
        "        h3 = article.find(\"h3\")\n",
        "        title_text = h3.get_text(strip=True) if h3 else \"Untitled Article\"\n",
        "        article_content = article.get_text(separator=\"\\n\", strip=True)\n",
        "        if not article_content.strip():\n",
        "            return None\n",
        "        cross_refs = _extract_cross_references(article, law_ref)\n",
        "        url = f\"{LOVDATA_BASE_URL}/{law_ref}#{article_id}\"\n",
        "        return LegalArticle(\n",
        "            article_id=article_id, title=title_text, content=article_content,\n",
        "            law_id=law_id, law_title=law_title_text, law_short_name=law_short_name,\n",
        "            chapter_id=chapter_id, chapter_title=chapter_title,\n",
        "            cross_references=cross_refs, url=url,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error processing article {idx} in {xml_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_xml_file(xml_path: Path) -> Iterator[LegalArticle]:\n",
        "    with open(xml_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "    if not content.strip():\n",
        "        return\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "    law_id = xml_path.stem\n",
        "    law_ref = _extract_law_ref_from_filename(law_id)\n",
        "    title_elem = soup.find(\"dd\", class_=\"title\") or soup.find(\"title\")\n",
        "    law_title = title_elem.get_text(strip=True) if title_elem else \"Unknown Law\"\n",
        "    law_short_name = _extract_short_name(soup)\n",
        "\n",
        "    sections = soup.find_all(\"section\", id=re.compile(r\"^kapittel-\\d+[a-zA-Z]?$\"))\n",
        "    if sections:\n",
        "        for section in sections:\n",
        "            chapter_id = section.get(\"id\", \"\")\n",
        "            h2 = section.find(\"h2\")\n",
        "            chapter_title = h2.get_text(strip=True) if h2 else None\n",
        "            if chapter_title:\n",
        "                match = re.match(r\"^Kapittel\\s+\\d+[A-Za-z]?\\.\\s*\", chapter_title)\n",
        "                if match:\n",
        "                    chapter_title = chapter_title[match.end():].strip() or chapter_title\n",
        "            for idx, art in enumerate(section.find_all(\"article\")):\n",
        "                result = _extract_article(art, idx, law_id, law_ref, law_title, law_short_name, chapter_id, chapter_title, xml_path)\n",
        "                if result:\n",
        "                    yield result\n",
        "    else:\n",
        "        for idx, art in enumerate(soup.find_all(\"article\")):\n",
        "            result = _extract_article(art, idx, law_id, law_ref, law_title, law_short_name, None, None, xml_path)\n",
        "            if result:\n",
        "                yield result\n",
        "\n",
        "print(\"Parser loaded.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parser override: use production parser implementation directly.\n",
        "from lovli.parser import LegalArticle, parse_law_header, parse_xml_file\n",
        "\n",
        "print(\"Parser override loaded from lovli.parser (single source of truth).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pre-index parser sanity check: verify production parser and required metadata fields.\n",
        "import lovli.parser as lp\n",
        "from pathlib import Path\n",
        "\n",
        "print(f\"Parser module path: {lp.__file__}\")\n",
        "\n",
        "sample = Path(\"data/nl/nl-19990326-017.xml\")\n",
        "if sample.exists():\n",
        "    sample_articles = list(parse_xml_file(sample))\n",
        "    assert sample_articles, \"Sample parse returned no articles\"\n",
        "    first = sample_articles[0]\n",
        "    assert hasattr(first, \"source_anchor_id\"), \"source_anchor_id missing; parser import is wrong\"\n",
        "    assert hasattr(first, \"doc_type\"), \"doc_type missing; parser import is wrong\"\n",
        "    assert first.doc_type in {\"provision\", \"editorial_note\"}, \"Unexpected doc_type value\"\n",
        "    print(\n",
        "        \"Parser sanity OK:\",\n",
        "        {\n",
        "            \"sample_articles\": len(sample_articles),\n",
        "            \"first_article_id\": first.article_id,\n",
        "            \"first_source_anchor_id\": first.source_anchor_id,\n",
        "            \"first_doc_type\": first.doc_type,\n",
        "        },\n",
        "    )\n",
        "else:\n",
        "    print(\"WARNING: sample file data/nl/nl-19990326-017.xml not found; skipping parser sanity parse.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Initialize Qdrant + Embedding Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "\n",
        "# Connect to Qdrant Cloud\n",
        "client = QdrantClient(\n",
        "    url=QDRANT_URL,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    timeout=QDRANT_TIMEOUT_SECONDS,\n",
        ")\n",
        "print(f\"Connected to Qdrant. Collections: {[c.name for c in client.get_collections().collections]}\")\n",
        "\n",
        "# Load embedding model on GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Loading {EMBEDDING_MODEL} on {device}...\")\n",
        "model = SentenceTransformer(EMBEDDING_MODEL, device=device)\n",
        "# FP16 for ~2x throughput on A100\n",
        "if device == \"cuda\":\n",
        "    model = model.half()\n",
        "print(f\"Model loaded on {device} (FP16). Embedding dim: {model.get_sentence_embedding_dimension()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Recreate collection (delete if exists)\n",
        "if client.collection_exists(COLLECTION_NAME):\n",
        "    print(f\"Deleting existing collection: {COLLECTION_NAME}\")\n",
        "    client.delete_collection(COLLECTION_NAME)\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    vectors_config=VectorParams(\n",
        "        size=EMBEDDING_DIMENSION,\n",
        "        distance=Distance.COSINE,\n",
        "    ),\n",
        "    on_disk_payload=True,\n",
        ")\n",
        "print(f\"Collection '{COLLECTION_NAME}' created (dense-only, payloads on disk)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Index All Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json\n",
        "\n",
        "\n",
        "def generate_id(law_id: str, source_anchor_id: str | None, article_id: str) -> int:\n",
        "    \"\"\"Stable 63-bit point ID based on law + stable source identity.\"\"\"\n",
        "    stable_source_id = source_anchor_id or article_id\n",
        "    key = f\"{law_id}::{stable_source_id}\"\n",
        "    hash_bytes = hashlib.sha256(key.encode(\"utf-8\")).digest()\n",
        "    return int.from_bytes(hash_bytes[:8], byteorder=\"big\", signed=False) % (2**63)\n",
        "\n",
        "\n",
        "def encode_texts_with_fallback(texts, batch_size):\n",
        "    \"\"\"Encode texts with OOM fallback: retry with batch 16, then CPU as last resort.\"\"\"\n",
        "    try:\n",
        "        return model.encode(texts, batch_size=batch_size, show_progress_bar=False, normalize_embeddings=True)\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" not in str(e).lower():\n",
        "            raise\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "        try:\n",
        "            return model.encode(texts, batch_size=16, show_progress_bar=False, normalize_embeddings=True)\n",
        "        except RuntimeError as e2:\n",
        "            if \"out of memory\" not in str(e2).lower():\n",
        "                raise\n",
        "            if device != \"cuda\":\n",
        "                raise\n",
        "            torch.cuda.empty_cache()\n",
        "            model_cpu = model.float().to(\"cpu\")\n",
        "            result = model_cpu.encode(texts, batch_size=32, show_progress_bar=False, normalize_embeddings=True)\n",
        "            model.to(device).half()\n",
        "            return result\n",
        "\n",
        "\n",
        "def upsert_with_retry(points, upsert_batch_size=INDEX_BATCH_SIZE):\n",
        "    \"\"\"Upsert to Qdrant with retry/backoff to handle transient write timeouts.\"\"\"\n",
        "    for i in range(0, len(points), upsert_batch_size):\n",
        "        batch = points[i:i + upsert_batch_size]\n",
        "        for attempt in range(1, UPSERT_MAX_RETRIES + 1):\n",
        "            try:\n",
        "                client.upsert(\n",
        "                    collection_name=COLLECTION_NAME,\n",
        "                    points=batch,\n",
        "                    wait=True,\n",
        "                )\n",
        "                break\n",
        "            except Exception as e:\n",
        "                is_last = attempt == UPSERT_MAX_RETRIES\n",
        "                if is_last:\n",
        "                    raise\n",
        "                sleep_s = UPSERT_BACKOFF_SECONDS * (2 ** (attempt - 1))\n",
        "                logger.warning(\n",
        "                    f\"Upsert retry {attempt}/{UPSERT_MAX_RETRIES} failed: {e}. \"\n",
        "                    f\"Sleeping {sleep_s}s...\"\n",
        "                )\n",
        "                time.sleep(sleep_s)\n",
        "\n",
        "\n",
        "def index_single_file(file_path: Path, embedding_batch_size=EMBEDDING_BATCH_SIZE, upsert_batch_size=INDEX_BATCH_SIZE):\n",
        "    \"\"\"Index a single XML file. Returns (article_count, error_or_none).\"\"\"\n",
        "    if file_path.name.startswith(\"._\"):\n",
        "        return 0, None\n",
        "\n",
        "    try:\n",
        "        # Parse articles\n",
        "        articles = list(parse_xml_file(file_path))\n",
        "        if not articles:\n",
        "            return 0, None\n",
        "\n",
        "        # Deduplicate by generated point ID (law_id + source_anchor_id fallback)\n",
        "        seen_ids = set()\n",
        "        unique_articles = []\n",
        "        for art in articles:\n",
        "            pid = generate_id(art.law_id, art.source_anchor_id, art.article_id)\n",
        "            if pid not in seen_ids:\n",
        "                seen_ids.add(pid)\n",
        "                unique_articles.append(art)\n",
        "        articles = unique_articles\n",
        "\n",
        "        # Generate embeddings with OOM fallback\n",
        "        texts = [a.content for a in articles]\n",
        "        all_embeddings = encode_texts_with_fallback(texts, embedding_batch_size)\n",
        "\n",
        "        # Build points\n",
        "        points = []\n",
        "        for idx, art in enumerate(articles):\n",
        "            points.append(PointStruct(\n",
        "                id=generate_id(art.law_id, art.source_anchor_id, art.article_id),\n",
        "                vector=all_embeddings[idx].tolist(),\n",
        "                payload={\n",
        "                    \"page_content\": art.content,\n",
        "                    \"metadata\": {\n",
        "                        \"article_id\": art.article_id,\n",
        "                        \"title\": art.title,\n",
        "                        \"law_id\": art.law_id,\n",
        "                        \"law_title\": art.law_title,\n",
        "                        \"law_short_name\": art.law_short_name,\n",
        "                        \"chapter_id\": art.chapter_id,\n",
        "                        \"chapter_title\": art.chapter_title,\n",
        "                        \"source_anchor_id\": art.source_anchor_id,\n",
        "                        \"doc_type\": art.doc_type,\n",
        "                        \"cross_references\": art.cross_references or [],\n",
        "                        \"url\": art.url,\n",
        "                    },\n",
        "                },\n",
        "            ))\n",
        "\n",
        "        upsert_with_retry(points, upsert_batch_size=upsert_batch_size)\n",
        "        return len(points), None\n",
        "\n",
        "    except Exception as e:\n",
        "        return 0, str(e)\n",
        "\n",
        "    finally:\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "def index_files(data_dir: str):\n",
        "    \"\"\"Index all XML files in a directory to Qdrant.\"\"\"\n",
        "    data_path = Path(data_dir)\n",
        "    if not data_path.is_dir():\n",
        "        print(f\"Skipping {data_dir} (not found)\")\n",
        "        return 0, 0, []\n",
        "\n",
        "    files = sorted(data_path.glob(\"*.xml\"))\n",
        "    total_files = len(files)\n",
        "    print(f\"\\nIndexing {total_files} files from {data_dir}\")\n",
        "\n",
        "    total_articles = 0\n",
        "    files_done = 0\n",
        "    failed = []\n",
        "    start = time.time()\n",
        "\n",
        "    for file_idx, file_path in enumerate(files):\n",
        "        article_count, err = index_single_file(file_path)\n",
        "        if err:\n",
        "            logger.error(f\"Failed {file_path.name}: {err}\")\n",
        "            failed.append({\"path\": str(file_path), \"error\": err})\n",
        "        else:\n",
        "            files_done += 1\n",
        "            total_articles += article_count\n",
        "\n",
        "        # Progress every 100 files\n",
        "        if (file_idx + 1) % 100 == 0 or file_idx == total_files - 1:\n",
        "            elapsed = time.time() - start\n",
        "            rate = (file_idx + 1) / elapsed if elapsed > 0 else 0\n",
        "            eta = (total_files - file_idx - 1) / rate if rate > 0 else 0\n",
        "            print(\n",
        "                f\"  [{file_idx + 1}/{total_files}] \"\n",
        "                f\"{total_articles} articles | \"\n",
        "                f\"{rate:.1f} files/s | \"\n",
        "                f\"ETA: {eta / 60:.0f}m\"\n",
        "            )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"\\nDone: {files_done}/{total_files} files, {total_articles} articles in {elapsed:.0f}s\")\n",
        "    if failed:\n",
        "        failed_names = [Path(x[\"path\"]).name for x in failed]\n",
        "        print(f\"Failed ({len(failed)}): {', '.join(failed_names[:10])}{'...' if len(failed_names) > 10 else ''}\")\n",
        "    return files_done, total_articles, failed\n",
        "\n",
        "\n",
        "# Run indexing for all directories\n",
        "grand_total_files = 0\n",
        "grand_total_articles = 0\n",
        "all_failed = []\n",
        "grand_start = time.time()\n",
        "\n",
        "for data_dir in DATA_DIRS:\n",
        "    files_done, articles, failed = index_files(data_dir)\n",
        "    grand_total_files += files_done\n",
        "    grand_total_articles += articles\n",
        "    all_failed.extend(failed)\n",
        "\n",
        "grand_elapsed = time.time() - grand_start\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INDEXING COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Total files:    {grand_total_files}\")\n",
        "print(f\"  Total articles: {grand_total_articles}\")\n",
        "print(f\"  Failed:         {len(all_failed)}\")\n",
        "print(f\"  Time:           {grand_elapsed / 60:.1f} minutes\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Persist failed records for optional backfill step\n",
        "FAILED_MANIFEST_PATH = \"/content/failed_files.json\"\n",
        "with open(FAILED_MANIFEST_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_failed, f, ensure_ascii=False, indent=2)\n",
        "print(f\"Saved failed-file manifest: {FAILED_MANIFEST_PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Backfill Failed Files (Optional)\n",
        "\n",
        "Run this after indexing if `failed_files.json` is non-empty. It retries only failed files with safer batch sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BACKFILL_EMBEDDING_BATCH_SIZE = 64\n",
        "BACKFILL_UPSERT_BATCH_SIZE = 150\n",
        "\n",
        "failed_path = Path(\"/content/failed_files.json\")\n",
        "if not failed_path.exists():\n",
        "    print(\"No failed manifest found. Run the indexing cell first.\")\n",
        "else:\n",
        "    with open(failed_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        failed_records = json.load(f)\n",
        "\n",
        "    if not failed_records:\n",
        "        print(\"No failed files to backfill.\")\n",
        "    else:\n",
        "        print(f\"Backfilling {len(failed_records)} failed files...\")\n",
        "        recovered = 0\n",
        "        still_failed = []\n",
        "        backfill_start = time.time()\n",
        "\n",
        "        for idx, rec in enumerate(failed_records, start=1):\n",
        "            file_path = Path(rec.get(\"path\", \"\"))\n",
        "            if not file_path.exists() and file_path.name:\n",
        "                # Fallback lookup by filename inside DATA_DIRS\n",
        "                candidates = [Path(d) / file_path.name for d in DATA_DIRS]\n",
        "                candidates = [p for p in candidates if p.exists()]\n",
        "                if candidates:\n",
        "                    file_path = candidates[0]\n",
        "\n",
        "            if not file_path.exists():\n",
        "                still_failed.append({\n",
        "                    \"path\": rec.get(\"path\", \"\"),\n",
        "                    \"error\": \"File not found during backfill\",\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            article_count, err = index_single_file(\n",
        "                file_path,\n",
        "                embedding_batch_size=BACKFILL_EMBEDDING_BATCH_SIZE,\n",
        "                upsert_batch_size=BACKFILL_UPSERT_BATCH_SIZE,\n",
        "            )\n",
        "\n",
        "            if err:\n",
        "                still_failed.append({\"path\": str(file_path), \"error\": err})\n",
        "            else:\n",
        "                recovered += 1\n",
        "\n",
        "            if idx % 10 == 0 or idx == len(failed_records):\n",
        "                elapsed = time.time() - backfill_start\n",
        "                print(f\"  [{idx}/{len(failed_records)}] recovered={recovered}, remaining_failed={len(still_failed)}, elapsed={elapsed:.0f}s\")\n",
        "\n",
        "        remaining_path = Path(\"/content/failed_files_remaining.json\")\n",
        "        with open(remaining_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(still_failed, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"\\nBACKFILL COMPLETE\")\n",
        "        print(f\"  Recovered: {recovered}\")\n",
        "        print(f\"  Still failed: {len(still_failed)}\")\n",
        "        print(f\"  Remaining manifest: {remaining_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Verify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "info = client.get_collection(COLLECTION_NAME)\n",
        "print(f\"Collection: {COLLECTION_NAME}\")\n",
        "print(f\"Points: {info.points_count}\")\n",
        "print(f\"Status: {info.status}\")\n",
        "\n",
        "# Validate metadata completeness for doc_type and show distribution sanity.\n",
        "missing_doc_type = 0\n",
        "provision_count = 0\n",
        "editorial_note_count = 0\n",
        "offset = None\n",
        "while True:\n",
        "    points, offset = client.scroll(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        limit=256,\n",
        "        offset=offset,\n",
        "        with_payload=True,\n",
        "        with_vectors=False,\n",
        "    )\n",
        "    if not points:\n",
        "        break\n",
        "    for point in points:\n",
        "        metadata = (point.payload or {}).get(\"metadata\", {}) or {}\n",
        "        doc_type = metadata.get(\"doc_type\")\n",
        "        if not doc_type:\n",
        "            missing_doc_type += 1\n",
        "        elif doc_type == \"provision\":\n",
        "            provision_count += 1\n",
        "        elif doc_type == \"editorial_note\":\n",
        "            editorial_note_count += 1\n",
        "    if offset is None:\n",
        "        break\n",
        "\n",
        "print(f\"missing_doc_type: {missing_doc_type}\")\n",
        "print(f\"provision_count: {provision_count}\")\n",
        "print(f\"editorial_note_count: {editorial_note_count}\")\n",
        "\n",
        "# Quick test search\n",
        "test_query = \"Hvor mye kan utleier kreve i depositum?\"\n",
        "test_embedding = model.encode(test_query).tolist()\n",
        "results = client.query_points(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    query=test_embedding,\n",
        "    limit=3,\n",
        ")\n",
        "print(f\"\\nTest query: '{test_query}'\")\n",
        "for i, point in enumerate(results.points):\n",
        "    meta = point.payload.get(\"metadata\", {})\n",
        "    print(\n",
        "        f\"  {i+1}. {meta.get('law_title', '?')} - {meta.get('title', '?')} \"\n",
        "        f\"[{meta.get('doc_type', '?')}] (score: {point.score:.3f})\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# (Deprecated duplicate parser import cell intentionally left blank.)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}