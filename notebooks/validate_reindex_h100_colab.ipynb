{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d2ab62ee",
      "metadata": {},
      "source": [
        "# Lovli Source-Gating Validation Run (Colab GPU)\n",
        "\n",
        "This notebook runs the **v3 source-gating workflow** on Colab (H100/T4 compatible), so we avoid local RAM limits.\n",
        "\n",
        "It runs:\n",
        "- **Merge pre-built catalogs** from Drive (`law_catalog_nl.json` + `law_catalog.json`) â†’ `data/law_catalog.json`\n",
        "- `scripts/validate_reindex.py`\n",
        "- `scripts/analyze_law_contamination.py`\n",
        "- `scripts/sweep_retrieval_thresholds.py`\n",
        "\n",
        "The setup enables law routing + law coherence filtering, includes reranker-context/routing-dualpass toggles, writes both sweep artifacts (`eval/retrieval_sweep_results.json` + `eval/retrieval_sweep_summary.json`), and exports files for review.\n",
        "\n",
        "## Run checklist\n",
        "\n",
        "| When | Mode | Action |\n",
        "|------|------|--------|\n",
        "| After each tuning change | `quick_iteration` | Use reduced sample + grid for fast feedback (~2-5 min) |\n",
        "| Before promotion / final decision | `full_validation` | Use full 98 questions + full grid + gate checks (~30-60 min) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0df554",
      "metadata": {},
      "source": [
        "## 1. Runtime and Repository Setup\n",
        "\n",
        "Use a **GPU runtime** before running this notebook (H100 preferred, T4 supported).\n",
        "\n",
        "If you cloned with an older commit, restart runtime and rerun from the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf lovli\n",
        "!git clone https://github.com/AndreasRamsli/lovli.git\n",
        "%cd /content/lovli\n",
        "\n",
        "# Install project with dependencies required by validation scripts.\n",
        "%pip install -q -U pip\n",
        "%pip install -q -e .\n",
        "\n",
        "# Safety net for environments where editable install path is delayed.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "src_path = str(Path('/content/lovli/src'))\n",
        "if src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "print('Setup complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f'GPU: {name}')\n",
        "    print(f'VRAM: {props.total_memory / (1024**3):.1f} GB')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b803b9c",
      "metadata": {},
      "source": [
        "## 2. Environment Configuration (v3 + routing/coherence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce9f311d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Required secrets: prompt securely in Colab.\n",
        "os.environ['QDRANT_URL'] = os.environ.get('QDRANT_URL') or input('QDRANT_URL: ').strip()\n",
        "os.environ['QDRANT_API_KEY'] = os.environ.get('QDRANT_API_KEY') or getpass.getpass('QDRANT_API_KEY: ').strip()\n",
        "os.environ['OPENROUTER_API_KEY'] = os.environ.get('OPENROUTER_API_KEY') or getpass.getpass('OPENROUTER_API_KEY: ').strip()\n",
        "\n",
        "# Collection defaults (override if needed).\n",
        "os.environ.setdefault('QDRANT_COLLECTION_NAME', 'lovli_laws_v3')\n",
        "\n",
        "# Keep traces off for speed/clean logs.\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
        "os.environ['LANGSMITH_TRACING'] = 'false'\n",
        "os.environ['SWEEP_SKIP_INDEX_SCAN'] = 'true'\n",
        "\n",
        "# Sweep promotion thresholds for new production gate checks.\n",
        "os.environ.setdefault('SWEEP_PROMOTION_MIN_IMPROVEMENT', '0.01')\n",
        "os.environ.setdefault('SWEEP_PROMOTION_PRECISION_TOLERANCE', '0.005')\n",
        "os.environ.setdefault('SWEEP_PROMOTION_NEGATIVE_TOLERANCE', '0.010')\n",
        "\n",
        "# Versioned trust profiles: switch TRUST_PROFILE between balanced_v1 and strict_v1.\n",
        "os.environ['TRUST_PROFILE_VERSION'] = '2026-02-17'\n",
        "profile_name = os.environ.get('TRUST_PROFILE', 'balanced_v1')\n",
        "profiles = {\n",
        "    'balanced_v1': {\n",
        "        'RETRIEVAL_K_INITIAL': '22',\n",
        "        'RERANKER_CONFIDENCE_THRESHOLD': '0.35',\n",
        "        'RERANKER_MIN_DOC_SCORE': '0.32',\n",
        "        'RERANKER_AMBIGUITY_MIN_GAP': '0.05',\n",
        "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
        "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'true',\n",
        "    },\n",
        "    'strict_v1': {\n",
        "        'RETRIEVAL_K_INITIAL': '15',\n",
        "        'RERANKER_CONFIDENCE_THRESHOLD': '0.45',\n",
        "        'RERANKER_MIN_DOC_SCORE': '0.55',\n",
        "        'RERANKER_AMBIGUITY_MIN_GAP': '0.10',\n",
        "        'RERANKER_AMBIGUITY_TOP_SCORE_CEILING': '0.7',\n",
        "        'LAW_ROUTING_FALLBACK_UNFILTERED': 'false',\n",
        "    },\n",
        "}\n",
        "profile = profiles.get(profile_name, profiles['balanced_v1'])\n",
        "os.environ['TRUST_PROFILE'] = profile_name if profile_name in profiles else 'balanced_v1'\n",
        "\n",
        "# Shared law routing/coherence + new dual-pass/reranker context controls.\n",
        "os.environ['LAW_ROUTING_ENABLED'] = 'true'\n",
        "os.environ['LAW_CATALOG_PATH'] = 'data/law_catalog.json'\n",
        "os.environ['LAW_ROUTING_PREFILTER_K'] = '80'\n",
        "os.environ['LAW_ROUTING_RERANK_TOP_K'] = '6'\n",
        "os.environ['LAW_ROUTING_MIN_CONFIDENCE'] = '0.30'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'] = '0.55'\n",
        "os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'] = '0.04'\n",
        "os.environ['LAW_ROUTING_FALLBACK_MAX_LAWS'] = '12'\n",
        "os.environ['LAW_COHERENCE_FILTER_ENABLED'] = 'true'\n",
        "os.environ['LAW_COHERENCE_MIN_LAW_COUNT'] = '2'\n",
        "os.environ['LAW_COHERENCE_SCORE_GAP'] = '0.15'\n",
        "os.environ['LAW_COHERENCE_RELATIVE_GAP'] = '0.05'\n",
        "os.environ['LAW_COHERENCE_MAX_SCORE_WEIGHT'] = '0.6'\n",
        "os.environ['LAW_COHERENCE_MIN_KEEP'] = '1'\n",
        "os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'] = '0.60'\n",
        "os.environ.setdefault('RERANKER_CONTEXT_ENRICHMENT_ENABLED', 'true')\n",
        "os.environ.setdefault('LAW_ROUTING_SUMMARY_DUALPASS_ENABLED', 'true')\n",
        "\n",
        "# Two-speed: set RUN_MODE='quick_iteration' for fast feedback, 'full_validation' for final check.\n",
        "os.environ.setdefault('RUN_MODE', 'full_validation')\n",
        "\n",
        "# Apply selected profile values.\n",
        "for key, value in profile.items():\n",
        "    os.environ[key] = value\n",
        "\n",
        "# Guard against accidental string values like 'None'.\n",
        "raw = os.environ.get('SWEEP_SAMPLE_SIZE')\n",
        "if raw is not None and raw.strip().lower() in {'', 'none', 'null'}:\n",
        "    os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "\n",
        "print('TRUST_PROFILE          =', os.environ['TRUST_PROFILE'])\n",
        "print('TRUST_PROFILE_VERSION  =', os.environ['TRUST_PROFILE_VERSION'])\n",
        "print('QDRANT_COLLECTION_NAME =', os.environ['QDRANT_COLLECTION_NAME'])\n",
        "print('LAW_ROUTING_ENABLED    =', os.environ['LAW_ROUTING_ENABLED'])\n",
        "print('LAW_CATALOG_PATH       =', os.environ['LAW_CATALOG_PATH'])\n",
        "print('LAW_ROUTING_PREFILTER  =', os.environ['LAW_ROUTING_PREFILTER_K'])\n",
        "print('LAW_ROUTING_RERANK_K   =', os.environ['LAW_ROUTING_RERANK_TOP_K'])\n",
        "print('LAW_ROUTING_CONF_MIN   =', os.environ['LAW_ROUTING_MIN_CONFIDENCE'])\n",
        "print('LAW_ROUTE_UNCERT_CEIL  =', os.environ['LAW_ROUTING_UNCERTAINTY_TOP_SCORE_CEILING'])\n",
        "print('LAW_ROUTE_UNCERT_GAP   =', os.environ['LAW_ROUTING_UNCERTAINTY_MIN_GAP'])\n",
        "print('LAW_ROUTE_FALLBACK     =', os.environ['LAW_ROUTING_FALLBACK_UNFILTERED'])\n",
        "print('LAW_COHERENCE_FILTER   =', os.environ['LAW_COHERENCE_FILTER_ENABLED'])\n",
        "print('LAW_COHERENCE_CONC_THR =', os.environ['LAW_COHERENCE_DOMINANT_CONCENTRATION_THRESHOLD'])\n",
        "print('RERANKER_CTX_ENRICH    =', os.environ['RERANKER_CONTEXT_ENRICHMENT_ENABLED'])\n",
        "print('ROUTING_DUALPASS       =', os.environ['LAW_ROUTING_SUMMARY_DUALPASS_ENABLED'])\n",
        "print('RUN_MODE               =', os.environ.get('RUN_MODE'))\n",
        "print('SWEEP_SAMPLE_SIZE      =', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85dcda35",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Two-speed workflow: quick_iteration (fast feedback) vs full_validation (final check).\n",
        "# Default changed to quick_iteration for faster iteration during testing/debugging.\n",
        "# Also available: SWEEP_DEBUG_MODE=true for ultra-fast focused threshold testing.\n",
        "RUN_MODE = os.environ.get('RUN_MODE', 'quick_iteration').strip().lower()  # or 'full_validation'\n",
        "DEBUG_MODE = os.environ.get('SWEEP_DEBUG_MODE', 'false').strip().lower() in ('1', 'true', 'yes')\n",
        "\n",
        "if RUN_MODE == 'quick_iteration':\n",
        "    if DEBUG_MODE:\n",
        "        # Ultra-fast: 15 questions, minimal grid for rapid threshold testing\n",
        "        os.environ['SWEEP_SAMPLE_SIZE'] = '15'\n",
        "        os.environ['CONTAMINATION_SAMPLE_SIZE'] = '15'\n",
        "        os.environ['SWEEP_QUICK_GRID'] = 'false'  # Use debug grid\n",
        "        os.environ['SWEEP_DEBUG_MODE'] = 'true'\n",
        "        os.environ['SKIP_VALIDATE_REINDEX'] = 'true'\n",
        "        os.environ['SWEEP_PARALLEL_PRECOMPUTE'] = 'true'\n",
        "        os.environ['SWEEP_PARALLEL_WORKERS'] = '6'\n",
        "        os.environ['SWEEP_CACHE_DIR'] = '/content/lovli/eval/sweep_cache'\n",
        "        os.environ['SWEEP_CHECKPOINT'] = 'true'\n",
        "        print('RUN_MODE = quick_iteration + DEBUG_MODE (sample=15, debug grid, 6 workers, cached precompute, skip validate)')\n",
        "    else:\n",
        "        os.environ['SWEEP_SAMPLE_SIZE'] = '20'\n",
        "        os.environ['CONTAMINATION_SAMPLE_SIZE'] = '20'\n",
        "        os.environ['SWEEP_QUICK_GRID'] = 'true'\n",
        "        os.environ['SKIP_VALIDATE_REINDEX'] = 'true'  # Skip smoke check for faster iteration\n",
        "        # GPU-accelerated reranker: parallel precompute now works well with multiple workers\n",
        "        # since the reranker runs on GPU (much lower RAM per worker than CPU-based model).\n",
        "        # With 80GB RAM + GPU, we can run 6 workers for true parallelism.\n",
        "        os.environ['SWEEP_PARALLEL_PRECOMPUTE'] = 'true'\n",
        "        os.environ['SWEEP_PARALLEL_WORKERS'] = '6'\n",
        "        # Cache precomputed candidates to /content/lovli/eval/sweep_cache so reruns\n",
        "        # skip the Qdrant + reranker step entirely when questions/commit haven't changed.\n",
        "        os.environ['SWEEP_CACHE_DIR'] = '/content/lovli/eval/sweep_cache'\n",
        "        os.environ['SWEEP_CHECKPOINT'] = 'true'\n",
        "        print('RUN_MODE = quick_iteration (sample=20, single combo, 6 workers, cached precompute, skip validate)')\n",
        "elif RUN_MODE == 'fast_debug':\n",
        "    # Ultra-fast: 10 questions, single combo\n",
        "    os.environ['SWEEP_SAMPLE_SIZE'] = '10'\n",
        "    os.environ['CONTAMINATION_SAMPLE_SIZE'] = '10'\n",
        "    os.environ['SWEEP_QUICK_GRID'] = 'true'\n",
        "    os.environ['SKIP_VALIDATE_REINDEX'] = 'true'\n",
        "    os.environ['SWEEP_PARALLEL_PRECOMPUTE'] = 'true'\n",
        "    os.environ['SWEEP_PARALLEL_WORKERS'] = '4'\n",
        "    os.environ['SWEEP_CACHE_DIR'] = '/content/lovli/eval/sweep_cache'\n",
        "    os.environ['SWEEP_CHECKPOINT'] = 'false'\n",
        "    print('RUN_MODE = fast_debug (sample=10, single combo, 4 workers, cached precompute, skip validate)')\n",
        "else:\n",
        "    os.environ.pop('SWEEP_SAMPLE_SIZE', None)\n",
        "    os.environ.pop('CONTAMINATION_SAMPLE_SIZE', None)\n",
        "    os.environ.pop('SWEEP_QUICK_GRID', None)\n",
        "    os.environ.pop('SWEEP_DEBUG_MODE', None)\n",
        "    os.environ.pop('SKIP_VALIDATE_REINDEX', None)\n",
        "    # Full validation: parallel precompute with GPU - can use 8 workers with 80GB RAM\n",
        "    os.environ['SWEEP_PARALLEL_PRECOMPUTE'] = 'true'\n",
        "    os.environ['SWEEP_PARALLEL_WORKERS'] = '8'\n",
        "    os.environ['SWEEP_CACHE_DIR'] = '/content/lovli/eval/sweep_cache'\n",
        "    os.environ['SWEEP_CHECKPOINT'] = 'true'\n",
        "    print('RUN_MODE = full_validation (full 98 questions, full grid, 8 workers)')\n",
        "\n",
        "print('SWEEP_SAMPLE_SIZE =', os.environ.get('SWEEP_SAMPLE_SIZE'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58b97dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preflight: create run envelope and clear stale artifacts/logs.\n",
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "import os\n",
        "\n",
        "run_id = datetime.now(timezone.utc).strftime('colab_%Y%m%dT%H%M%SZ')\n",
        "os.environ['LOVLI_RUN_ID'] = run_id\n",
        "print('LOVLI_RUN_ID =', run_id)\n",
        "\n",
        "preflight_targets = [\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "    Path('eval/retrieval_sweep_summary.json'),\n",
        "    Path('eval/logs/analyze_law_contamination.log'),\n",
        "    Path('eval/logs/retrieval_sweep_quick.log'),\n",
        "    Path('eval/logs/retrieval_sweep_full.log'),\n",
        "    Path('eval/logs/regression_gates.log'),\n",
        "]\n",
        "for target in preflight_targets:\n",
        "    if target.exists():\n",
        "        target.unlink()\n",
        "        print('removed', target)\n",
        "    else:\n",
        "        print('missing', target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce068d4d",
      "metadata": {},
      "source": [
        "## 3. Mount Drive, Extract Data, Merge Catalog, Validate Reindex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f478d940",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/lovli\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def run_to_log(cmd: str, log_path: Path) -> int:\n",
        "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(log_path, 'w', encoding='utf-8') as log_file:\n",
        "        proc = subprocess.run(\n",
        "            cmd,\n",
        "            shell=True,\n",
        "            cwd='/content/lovli',\n",
        "            stdout=log_file,\n",
        "            stderr=subprocess.STDOUT,\n",
        "            text=True,\n",
        "        )\n",
        "    return proc.returncode\n",
        "\n",
        "\n",
        "def print_log_matches(log_path: Path, patterns: list[str], limit: int = 50) -> None:\n",
        "    if not log_path.exists():\n",
        "        print(f'log missing: {log_path}')\n",
        "        return\n",
        "    lines = log_path.read_text(encoding='utf-8', errors='ignore').splitlines()\n",
        "    kept = []\n",
        "    for line in lines:\n",
        "        if any(p in line for p in patterns):\n",
        "            kept.append(line)\n",
        "    print(f'--- {log_path.name} (key lines) ---')\n",
        "    for line in kept[-limit:]:\n",
        "        print(line)\n",
        "\n",
        "\n",
        "# Mount Drive for access to the compressed dataset.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Skip unchanged prep when possible (set FORCE_REFRESH=true to always run).\n",
        "FORCE_REFRESH = os.environ.get('FORCE_REFRESH', 'false').strip().lower() in ('1', 'true', 'yes')\n",
        "SKIP_VALIDATE_REINDEX = os.environ.get('SKIP_VALIDATE_REINDEX', 'false').strip().lower() in ('1', 'true', 'yes')\n",
        "\n",
        "# Update this path if your tar or catalogs are moved.\n",
        "drive_data = Path('/content/drive/MyDrive/Colab Notebooks/Lovli/data')\n",
        "tar_path = drive_data / 'lovli-data.tar.bz2'\n",
        "catalog_nl_path = drive_data / 'law_catalog_nl.json'\n",
        "catalog_sf_path = drive_data / 'law_catalog.json'\n",
        "\n",
        "assert tar_path.exists(), f'Data tar not found: {tar_path}'\n",
        "assert catalog_nl_path.exists(), f'law_catalog_nl.json not found; upload to {drive_data}'\n",
        "assert catalog_sf_path.exists(), f'law_catalog.json not found; upload to {drive_data}'\n",
        "\n",
        "# Extract into repo data/ folder (skip if data already present and not forcing).\n",
        "nl_dir = Path('/content/lovli/data/nl')\n",
        "sf_dir = Path('/content/lovli/data/sf')\n",
        "need_extract = FORCE_REFRESH or not nl_dir.exists() or not sf_dir.exists()\n",
        "if not need_extract:\n",
        "    nl_count_pre = len(list(nl_dir.glob('*.xml')))\n",
        "    sf_count_pre = len(list(sf_dir.glob('*.xml')))\n",
        "    if nl_count_pre == 0 or sf_count_pre == 0:\n",
        "        need_extract = True\n",
        "if need_extract:\n",
        "    subprocess.run(\"mkdir -p /content/lovli/data\", shell=True, check=True)\n",
        "    subprocess.run(\n",
        "        f\"tar -xjf '{tar_path}' -C /content/lovli --exclude='._*'\",\n",
        "        shell=True,\n",
        "        check=True,\n",
        "    )\n",
        "    print('Extracted data from tar.')\n",
        "else:\n",
        "    print('Skipped extraction (data already present). Set FORCE_REFRESH=true to re-extract.')\n",
        "\n",
        "nl_count = len(list(nl_dir.glob('*.xml'))) if nl_dir.exists() else 0\n",
        "sf_count = len(list(sf_dir.glob('*.xml'))) if sf_dir.exists() else 0\n",
        "print({'nl_xml_files': nl_count, 'sf_xml_files': sf_count})\n",
        "assert nl_count > 0 and sf_count > 0, 'Expected both data/nl and data/sf to contain XML files.'\n",
        "\n",
        "# Merge pre-built catalogs (skip if output newer than sources).\n",
        "output_catalog = Path('/content/lovli/data/law_catalog.json')\n",
        "need_merge = FORCE_REFRESH or not output_catalog.exists()\n",
        "if output_catalog.exists() and catalog_nl_path.exists() and catalog_sf_path.exists():\n",
        "    out_mtime = output_catalog.stat().st_mtime\n",
        "    if out_mtime >= catalog_nl_path.stat().st_mtime and out_mtime >= catalog_sf_path.stat().st_mtime:\n",
        "        need_merge = False\n",
        "if need_merge:\n",
        "    nl_catalog = json.loads(catalog_nl_path.read_text(encoding='utf-8'))\n",
        "    sf_catalog = json.loads(catalog_sf_path.read_text(encoding='utf-8'))\n",
        "    seen = set()\n",
        "    merged = []\n",
        "    for entry in nl_catalog + sf_catalog:\n",
        "        law_id = (entry.get('law_id') or '').strip()\n",
        "        if law_id and law_id not in seen:\n",
        "            seen.add(law_id)\n",
        "            merged.append(entry)\n",
        "    output_catalog.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(output_catalog, 'w', encoding='utf-8') as f:\n",
        "        json.dump(merged, f, ensure_ascii=False, indent=2)\n",
        "    nl_with_summary = sum(1 for e in nl_catalog if e.get('summary'))\n",
        "    sf_with_summary = sum(1 for e in sf_catalog if e.get('summary'))\n",
        "    print(f'Merged catalog: {len(merged)} entries (nl={len(nl_catalog)}, sf={len(sf_catalog)})')\n",
        "    print(f'With summaries: nl={nl_with_summary}, sf={sf_with_summary}')\n",
        "else:\n",
        "    print('Skipped catalog merge (output newer than sources).')\n",
        "\n",
        "# Validate metadata + retrieval smoke checks (skip when SKIP_VALIDATE_REINDEX=true).\n",
        "if SKIP_VALIDATE_REINDEX:\n",
        "    print('Skipped validate_reindex (SKIP_VALIDATE_REINDEX=true).')\n",
        "    rc = 0\n",
        "else:\n",
        "    validate_log = Path('/content/lovli/eval/logs/validate_reindex.log')\n",
        "    rc = run_to_log(\n",
        "        'python scripts/validate_reindex.py --collection lovli_laws_v3 --with-smoke',\n",
        "        validate_log,\n",
        "    )\n",
        "    print('validate_reindex exit_code =', rc)\n",
        "    print_log_matches(\n",
        "        validate_log,\n",
        "        patterns=['Collection:', 'total_points=', 'missing_doc_type=', 'smoke query=', 'Validation completed.'],\n",
        "    )\n",
        "    assert rc == 0, 'validate_reindex failed; inspect eval/logs/validate_reindex.log'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "308b8d5c",
      "metadata": {},
      "source": [
        "## 4. Law Contamination Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6db4a328",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "contam_log = Path('/content/lovli/eval/logs/analyze_law_contamination.log')\n",
        "rc = run_to_log(\n",
        "    'python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json',\n",
        "    contam_log,\n",
        ")\n",
        "print('analyze_law_contamination exit_code =', rc)\n",
        "print_log_matches(\n",
        "    contam_log,\n",
        "    patterns=['Processed', 'Saved contamination report', 'Contamination rate='],\n",
        ")\n",
        "\n",
        "report_path = Path('/content/lovli/eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('--- contamination aggregate ---')\n",
        "    for key in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'{key}: {agg.get(key)}')\n",
        "\n",
        "assert rc == 0, 'analyze_law_contamination failed; inspect eval/logs/analyze_law_contamination.log'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09926968",
      "metadata": {},
      "source": [
        "## 5. Full Retrieval Sweep (Colab run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e31d5ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/lovli\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "full_sweep_log = Path('/content/lovli/eval/logs/retrieval_sweep_full.log')\n",
        "rc = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', full_sweep_log)\n",
        "\n",
        "print('full sweep exit_code =', rc)\n",
        "print_log_matches(\n",
        "    full_sweep_log,\n",
        "    patterns=['Saved results:', 'Saved ablation summary:', 'Promotion gate summary:', 'Top 5 configurations:'],\n",
        ")\n",
        "\n",
        "sweep_path = Path('/content/lovli/eval/retrieval_sweep_results.json')\n",
        "summary_path = Path('/content/lovli/eval/retrieval_sweep_summary.json')\n",
        "\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('--- full sweep top row ---')\n",
        "        for key in [\n",
        "            'is_profile_default_row',\n",
        "            'recall_at_k',\n",
        "            'recall_at_1',\n",
        "            'recall_at_3',\n",
        "            'recall_at_5',\n",
        "            'mrr_at_5',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'source_boundary_mismatch_at_k',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "            'promotion_gate_pass',\n",
        "            'balanced_score',\n",
        "        ]:\n",
        "            print(f'{key}: {top.get(key)}')\n",
        "\n",
        "if summary_path.exists():\n",
        "    summary = json.loads(summary_path.read_text(encoding='utf-8'))\n",
        "    print('--- ablation summary ---')\n",
        "    for key in ['run_id', 'rows_count', 'promotion_gate_pass_count', 'promotion_gate_total']:\n",
        "        print(f'{key}: {summary.get(key)}')\n",
        "\n",
        "assert rc == 0, 'full sweep failed; inspect eval/logs/retrieval_sweep_full.log'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6420a064",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifact metadata compatibility check before regression gates.\n",
        "%cd /content/lovli\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "report_path = Path('eval/law_contamination_report.json')\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "summary_path = Path('eval/retrieval_sweep_summary.json')\n",
        "\n",
        "assert report_path.exists(), 'Missing contamination report artifact'\n",
        "assert sweep_path.exists(), 'Missing sweep results artifact'\n",
        "assert summary_path.exists(), 'Missing sweep summary artifact'\n",
        "\n",
        "report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "summary = json.loads(summary_path.read_text(encoding='utf-8'))\n",
        "assert rows, 'Sweep results are empty'\n",
        "\n",
        "contam_meta = report.get('artifact_metadata', {})\n",
        "sweep_meta = {\n",
        "    'run_id': rows[0].get('run_id'),\n",
        "    'git_commit': rows[0].get('git_commit'),\n",
        "    'questions_sha256': rows[0].get('questions_sha256'),\n",
        "    'question_count': rows[0].get('question_count'),\n",
        "}\n",
        "summary_meta = {\n",
        "    'run_id': summary.get('run_id'),\n",
        "    'git_commit': summary.get('git_commit'),\n",
        "    'questions_sha256': summary.get('questions_sha256'),\n",
        "    'question_count': summary.get('question_count'),\n",
        "}\n",
        "\n",
        "print('contamination metadata:', contam_meta)\n",
        "print('sweep metadata:', sweep_meta)\n",
        "print('summary metadata:', summary_meta)\n",
        "\n",
        "for key in ['run_id', 'git_commit', 'questions_sha256', 'question_count']:\n",
        "    left = contam_meta.get(key)\n",
        "    right = sweep_meta.get(key)\n",
        "    if left is None or right is None:\n",
        "        print(f'skip metadata check for {key}: value missing')\n",
        "        continue\n",
        "    assert str(left) == str(right), (\n",
        "        f'Artifact mismatch for {key}: contamination={left} sweep={right}'\n",
        "    )\n",
        "\n",
        "for key in ['run_id', 'git_commit', 'questions_sha256', 'question_count']:\n",
        "    left = sweep_meta.get(key)\n",
        "    right = summary_meta.get(key)\n",
        "    if left is None or right is None:\n",
        "        print(f'skip summary metadata check for {key}: value missing')\n",
        "        continue\n",
        "    assert str(left) == str(right), (\n",
        "        f'Artifact mismatch for {key}: sweep={left} summary={right}'\n",
        "    )\n",
        "\n",
        "print('Artifact metadata compatibility check passed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b098501",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Acceptance checks + focused debug for fallback-stage route misses.\n",
        "%cd /content/lovli\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "contam_path = Path('eval/law_contamination_report.json')\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "assert contam_path.exists(), 'Missing eval/law_contamination_report.json'\n",
        "assert sweep_path.exists(), 'Missing eval/retrieval_sweep_results.json'\n",
        "\n",
        "contam = json.loads(contam_path.read_text(encoding='utf-8'))\n",
        "rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "assert rows, 'Sweep results are empty'\n",
        "\n",
        "profile_name = (contam.get('trust_profile_name') or '').strip()\n",
        "profile_rows = [r for r in rows if r.get('trust_profile_name') == profile_name] if profile_name else rows\n",
        "assert profile_rows, f'No sweep rows found for profile={profile_name!r}'\n",
        "default_rows = [r for r in profile_rows if bool(r.get('is_profile_default_row'))]\n",
        "assert len(default_rows) == 1, f'Expected one profile default row, found {len(default_rows)}'\n",
        "selected = default_rows[0]\n",
        "\n",
        "agg = contam.get('aggregate', {})\n",
        "for required_key in [\n",
        "    'fallback_stage_counts',\n",
        "    'route_miss_by_fallback_stage',\n",
        "    'route_miss_rate_by_stage',\n",
        "    'fallback_recovery_rate_by_stage',\n",
        "    'route_miss_count_by_mode_stage',\n",
        "]:\n",
        "    assert required_key in agg, f'Missing aggregate metric: {required_key}'\n",
        "\n",
        "print('=== Gate-selected default row ===')\n",
        "for k in [\n",
        "    'is_profile_default_row',\n",
        "    'retrieval_k_initial',\n",
        "    'retrieval_k',\n",
        "    'reranker_confidence_threshold',\n",
        "    'reranker_min_doc_score',\n",
        "    'law_routing_fallback_unfiltered',\n",
        "    'recall_at_k',\n",
        "    'citation_precision',\n",
        "    'unexpected_citation_rate',\n",
        "    'false_positive_gating_rate',\n",
        "    'balanced_score',\n",
        "    'routing_uncertain_count',\n",
        "    'fallback_stage1_accepted_count',\n",
        "    'fallback_stage2_unfiltered_count',\n",
        "]:\n",
        "    print(f'{k}: {selected.get(k)}')\n",
        "\n",
        "print('\\n=== Route miss by fallback stage ===')\n",
        "print('counts:', agg.get('route_miss_by_fallback_stage'))\n",
        "print('rates :', agg.get('route_miss_rate_by_stage'))\n",
        "\n",
        "print('\\n=== Fallback recovery by stage ===')\n",
        "print(agg.get('fallback_recovery_rate_by_stage'))\n",
        "\n",
        "print('\\n=== Top route-miss law pair confusions ===')\n",
        "for row in (agg.get('top_route_miss_law_pair_confusions') or [])[:10]:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f4bacf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Controlled rerun envelope: contamination -> sweep -> gates (v1,v2).\n",
        "%cd /content/lovli\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "run_id = os.environ.get('LOVLI_RUN_ID') or datetime.now(timezone.utc).strftime('colab_%Y%m%dT%H%M%SZ')\n",
        "os.environ['LOVLI_RUN_ID'] = run_id\n",
        "profile_for_gates = (os.environ.get('TRUST_PROFILE') or 'balanced_v1').strip() or 'balanced_v1'\n",
        "os.environ['TRUST_PROFILE'] = profile_for_gates\n",
        "print('Controlled run envelope:', run_id)\n",
        "print('Gate profile:', profile_for_gates)\n",
        "\n",
        "logs_dir = Path('eval/logs')\n",
        "logs_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "contam_log = logs_dir / 'analyze_law_contamination.log'\n",
        "sweep_log = logs_dir / 'retrieval_sweep_full.log'\n",
        "gates_v1_log = logs_dir / 'regression_gates_v1.log'\n",
        "gates_v2_log = logs_dir / 'regression_gates_v2.log'\n",
        "\n",
        "base_gate_cmd = (\n",
        "    'python -u scripts/check_regression_gates.py '\n",
        "    '--contamination-report eval/law_contamination_report.json '\n",
        "    '--sweep-results eval/retrieval_sweep_results.json '\n",
        "    '--baseline eval/baselines/production_trust_baseline_v1.json '\n",
        "    f'--profile \"{profile_for_gates}\" '\n",
        ")\n",
        "\n",
        "rc_contam = run_to_log(\n",
        "    'python -u scripts/analyze_law_contamination.py --output eval/law_contamination_report.json',\n",
        "    contam_log,\n",
        ")\n",
        "rc_sweep = run_to_log('python -u scripts/sweep_retrieval_thresholds.py', sweep_log)\n",
        "rc_v1 = run_to_log(base_gate_cmd + '--gate-tier v1', gates_v1_log)\n",
        "rc_v2 = run_to_log(base_gate_cmd + '--gate-tier v2', gates_v2_log)\n",
        "\n",
        "print('exit codes:', {\n",
        "    'contamination': rc_contam,\n",
        "    'sweep': rc_sweep,\n",
        "    'gates_v1': rc_v1,\n",
        "    'gates_v2': rc_v2,\n",
        "})\n",
        "\n",
        "for path in [contam_log, sweep_log, gates_v1_log, gates_v2_log]:\n",
        "    assert path.exists(), f'Missing log: {path}'\n",
        "\n",
        "print_log_matches(contam_log, patterns=['Run metadata:', 'Saved contamination report', 'Gate summary:'])\n",
        "print_log_matches(sweep_log, patterns=['Run metadata:', 'Parity debug:', 'Parity divergence counts:', 'Saved results:'])\n",
        "print_log_matches(gates_v1_log, patterns=['Artifact metadata:', 'Gate checks passed', 'Regression gates failed', 'All regression gates passed'])\n",
        "print_log_matches(gates_v2_log, patterns=['Artifact metadata:', 'Gate checks passed', 'Regression gates failed', 'All regression gates passed'])\n",
        "\n",
        "report = json.loads(Path('eval/law_contamination_report.json').read_text(encoding='utf-8'))\n",
        "rows = json.loads(Path('eval/retrieval_sweep_results.json').read_text(encoding='utf-8'))\n",
        "assert rows, 'Sweep results are empty'\n",
        "profile = report.get('trust_profile_name')\n",
        "default_rows = [r for r in rows if r.get('trust_profile_name') == profile and bool(r.get('is_profile_default_row'))]\n",
        "assert len(default_rows) == 1, f'Expected one default row for profile={profile}, got {len(default_rows)}'\n",
        "default_row = default_rows[0]\n",
        "\n",
        "print('\\n=== Controlled run assessment (profile default row) ===')\n",
        "for key in ['recall_at_k', 'citation_precision', 'unexpected_citation_rate', 'false_positive_gating_rate', 'balanced_score', 'fallback_stage1_accepted_count', 'fallback_stage2_unfiltered_count']:\n",
        "    print(f'{key}: {default_row.get(key)}')\n",
        "\n",
        "agg = report.get('aggregate', {})\n",
        "for key in ['route_miss_expected_law_rate', 'dominant_law_mismatch_rate', 'fallback_recovery_rate', 'route_miss_by_fallback_stage', 'route_miss_rate_by_stage']:\n",
        "    print(f'{key}: {agg.get(key)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf086b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Must-pass regression gates against versioned baseline.\n",
        "# Note: controlled rerun cell above already runs v1/v2; keep this as a standalone quick v1 check.\n",
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "\n",
        "profile_for_gates = (os.environ.get('TRUST_PROFILE') or 'balanced_v1').strip() or 'balanced_v1'\n",
        "os.environ['TRUST_PROFILE'] = profile_for_gates\n",
        "print('Gate profile:', profile_for_gates)\n",
        "\n",
        "gates_log = Path('/content/lovli/eval/logs/regression_gates.log')\n",
        "rc = run_to_log(\n",
        "    'python -u scripts/check_regression_gates.py '\n",
        "    '--contamination-report eval/law_contamination_report.json '\n",
        "    '--sweep-results eval/retrieval_sweep_results.json '\n",
        "    '--baseline eval/baselines/production_trust_baseline_v1.json '\n",
        "    f'--profile \"{profile_for_gates}\" '\n",
        "    '--gate-tier v1',\n",
        "    gates_log,\n",
        ")\n",
        "print('regression gates exit_code =', rc)\n",
        "print_log_matches(\n",
        "    gates_log,\n",
        "    patterns=['Artifact metadata:', 'Gate sweep row selected', '[PASS]', '[FAIL]', 'Gate checks passed', 'All regression gates passed'],\n",
        "    limit=120,\n",
        ")\n",
        "assert rc == 0, 'regression gates failed; inspect eval/logs/regression_gates.log'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a061fef",
      "metadata": {},
      "source": [
        "## 6. Artifact Overview and Quick Metric Check\n",
        "\n",
        "Run acceptance targets (balanced objective):\n",
        "- `recall_at_k` should improve materially vs previous baseline (~0.146)\n",
        "- `citation_precision` should increase from previous baseline (~0.073)\n",
        "- `unexpected_citation_rate` should decrease\n",
        "- `law_coherence_filtered_count` should be non-zero on full sweep\n",
        "- `missing_doc_type` must remain `0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d1009eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/lovli\n",
        "!ls -lah eval\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "artifacts = [\n",
        "    Path('data/law_catalog.json'),\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "    Path('eval/retrieval_sweep_summary.json'),\n",
        "]\n",
        "for p in artifacts:\n",
        "    print(f'{p}:', 'exists' if p.exists() else 'missing')\n",
        "\n",
        "report_path = Path('eval/law_contamination_report.json')\n",
        "if report_path.exists():\n",
        "    report = json.loads(report_path.read_text(encoding='utf-8'))\n",
        "    agg = report.get('aggregate', {})\n",
        "    print('\\nContamination aggregate:')\n",
        "    for k in [\n",
        "        'total_questions',\n",
        "        'contamination_rate',\n",
        "        'singleton_foreign_rate',\n",
        "        'unexpected_citation_rate',\n",
        "        'mean_foreign_score_gap',\n",
        "    ]:\n",
        "        print(f'  {k}: {agg.get(k)}')\n",
        "\n",
        "sweep_path = Path('eval/retrieval_sweep_results.json')\n",
        "if sweep_path.exists():\n",
        "    rows = json.loads(sweep_path.read_text(encoding='utf-8'))\n",
        "    if rows:\n",
        "        top = rows[0]\n",
        "        print('\\nTop sweep row:')\n",
        "        for k in [\n",
        "            'recall_at_k',\n",
        "            'recall_at_1',\n",
        "            'recall_at_3',\n",
        "            'recall_at_5',\n",
        "            'mrr_at_5',\n",
        "            'f1_at_k',\n",
        "            'citation_precision',\n",
        "            'unexpected_citation_rate',\n",
        "            'source_boundary_mismatch_at_k',\n",
        "            'law_contamination_rate',\n",
        "            'law_coherence_filtered_count',\n",
        "            'promotion_gate_pass',\n",
        "        ]:\n",
        "            print(f'  {k}: {top.get(k)}')\n",
        "\n",
        "summary_path = Path('eval/retrieval_sweep_summary.json')\n",
        "if summary_path.exists():\n",
        "    summary = json.loads(summary_path.read_text(encoding='utf-8'))\n",
        "    print('\\nSweep summary:')\n",
        "    for k in [\n",
        "        'run_id',\n",
        "        'rows_count',\n",
        "        'promotion_gate_pass_count',\n",
        "        'promotion_gate_total',\n",
        "    ]:\n",
        "        print(f'  {k}: {summary.get(k)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b1da59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: download key artifacts from Colab runtime.\n",
        "%cd /content/lovli\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "download_targets = [\n",
        "    Path('eval/law_contamination_report.json'),\n",
        "    Path('eval/retrieval_sweep_results.json'),\n",
        "    Path('eval/retrieval_sweep_summary.json'),\n",
        "    Path('eval/logs/retrieval_sweep_full.log'),\n",
        "]\n",
        "\n",
        "for target in download_targets:\n",
        "    if target.exists():\n",
        "        print('downloading', target)\n",
        "        files.download(str(target))\n",
        "    else:\n",
        "        print('missing', target)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "validate_reindex_h100_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
